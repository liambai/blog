---
title: "Protein language models through the logit lens"
date: "2025-05-21"
description: Applying the logit lens to ESM-2. Logit visualization & interpretation, attention analysis, looking inside the mind of a protein language model.
---

import { Link } from "gatsby"
import Figure from "../../../src/components/figure.jsx"
import Image from "../../../src/components/image.jsx"
import TopTokensHeatmap from "./d3/top_tokens_heatmap"
import TrueTokensRanksHeatmap from "./d3/true_tokens_ranks_heatmap"
import StructureOverlay from "./d3/structure_overlay"

The [logit lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) is a powerful tool for interpreting LLMs. Can we use it to better understand protein language models?

## The logit lens

Protein language models like [ESM-2](https://github.com/facebookresearch/esm) are trained with the masked token prediction task. Given a protein sequence:

$$
\text{Q V Q L V [?] S G A}
$$

What is the amino acid at the masked position?

ESM answers this question with 20 numbers (**logits**), one for each possible amino acid. Each logit indicates ESM's confidence level in that amino acid being the masked one. To make a prediction, we pick the amino acid with the highest logit.

Here's the idea: logits can be calculated for not only the last layer (ESM's final answers), but also intermediate layers. Intermediate logits give a view into the model's information flow, and in some limited sense, its "thought process".

## ESM through the logit lens

### Beta-lactamase

I took a [beta-lactamase](https://en.wikipedia.org/wiki/Beta-lactamase) sequence, masked each position one at a time, and calculated the logits across each layer of [ESM-2 (650M)](https://huggingface.co/facebook/esm2_t33_650M_UR50D):

<TopTokensHeatmap
  title="Beta-lactamase (PDB 4ZAM) top tokens by logit"
  sequence="SPQPLEQIKLSESQLSGRVGMIEMDLASGRTLTAWRADERFPMMSTFKVVLCGAVLARVDAGDEQLERKIHYRQQDLVDYSPVSEKHLADGMTVGELCAAAITMSDNSAANLLLATVGGPAGLTAFLRQIGDNVTRLDRWETELNEALPGDARDTTTPASMAATLRKLLTSQRLSARSQRQLLQWMVDDRVAGPLIRSVLPAGWFIADKTGAGERGARGIVALLGPNNKAERIVVIYLRDTPASMAERNQQIAGIGAALIEHWQR"
  tokensPath="/data/logit-lens/beta_lactamase_top_tokens.csv"
  logitsPath="/data/logit-lens/beta_lactamase_top_logits.csv"
  maxLogit="12.35"
/>

Each cell shows the amino acid that ESM is most confident in, colored by its logit value (scroll right for more positions, mouseover for logit values). The true amino acid sequence is shown at the bottom, where the ones that don't match ESM's final prediction are red.

- Logits in earlier layers tend to be wrong. As we move through the layers, ESM often converges on the right answer, but not always.
- By logit values, ESM clearly believes in some positions more than others. For example, it's super confident in position 45 being S––and it's right! As it turns out, the S at position 45 constitutes a binding site, which means that it is likely highly conserved.

<Figure content={<Image path={require("./images/beta-lactamase-45.png")} />}>
  Beta-lactamse (PDB [4ZAM](https://www.rcsb.org/3d-sequence/4ZAM?asymId=A)) has
  a binding site annotation at position 45. We can see on the right that this
  position contacts the ligand and is therefore likely highly conserved.
</Figure>

- Similarly, ESM also believes strongly––and correctly––in the D at position 106, another binding site. You can explore more annotations at [https://www.rcsb.org/3d-sequence/4ZAM?asymId=A](https://www.rcsb.org/3d-sequence/4ZAM?asymId=A).

<Figure content={<Image path={require("./images/beta-lactamase-106.png")} />}>
  Beta-lactamse (PDB [4ZAM](https://www.rcsb.org/3d-sequence/4ZAM?asymId=A)) has
  another binding site annotation at position 106.
</Figure>

- At the first position, ESM is wrong but made a reasonable guess: [Methionine (M)](https://en.wikipedia.org/wiki/Methionine) corresponds to the start codon so most proteins start with it.

- Sometimes, ESM starts believing in an amino acid in an early layer (e.g. position 29 starting from layer 14). Sometimes, it "changes its mind" at the last layer (position 15).

Here's a visualization of the top logit values at each position overlaid on the protein's structure. Use the slider to adjust the layer.

<StructureOverlay
  title="Beta-lactamase (PDB 4ZAM) structure colored by top logit"
  pdbId="4ZAM"
  logitsPath="/data/logit-lens/beta_lactamase_top_logits.csv"
  maxLogit="12.35"
/>

Of course, focusing on the top amino acid is limiting. What about the other amino acids? If ESM got the final prediction wrong, did it come close by at least assigning the true amino acid _one of_ the highest logits? We can answer that by plotting the rank of the true amino acid among the 20 options.

<TrueTokensRanksHeatmap
  title="Beta-lactamase (PDB 4ZAM) true token ranks"
  sequence="SPQPLEQIKLSESQLSGRVGMIEMDLASGRTLTAWRADERFPMMSTFKVVLCGAVLARVDAGDEQLERKIHYRQQDLVDYSPVSEKHLADGMTVGELCAAAITMSDNSAANLLLATVGGPAGLTAFLRQIGDNVTRLDRWETELNEALPGDARDTTTPASMAATLRKLLTSQRLSARSQRQLLQWMVDDRVAGPLIRSVLPAGWFIADKTGAGERGARGIVALLGPNNKAERIVVIYLRDTPASMAERNQQIAGIGAALIEHWQR"
  ranksPath="/data/logit-lens/beta_lactamase_true_token_ranks.csv"
/>

In many cases where ESM made the wrong prediction, the correct amino acid was quite highly ranked. It got so close! For example, at position 5, the correct amino acid corresponds to ESM's second highest logit.

### Antibody

I repeated this for an [antibody heavy chain](https://en.wikipedia.org/wiki/Immunoglobulin_heavy_chain) sequence.

<TopTokensHeatmap
  title="Antibody heavy chain (PDB 5XRQ) top tokens by logit"
  sequence="QVQLVQSGAEVKKPGSSVRVSCKASGDTFSSYSITWVRQAPGHGLQWMGGIFPIFGSTNYAQKFDDRLTITTDDSSRTVYMELTSLRLEDTAVYYCARGASKVEPAAPAYSDAFDMWGQGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKRVEPKSCHHHHHH"
  tokensPath="/data/logit-lens/ab_heavy_chain_top_tokens.csv"
  logitsPath="/data/logit-lens/ab_heavy_chain_top_logits.csv"
  maxLogit="12"
/>

I noticed ESM's high conviction on positions 22 and 96 being C. They form a [disulfide bridge](https://www.creative-proteomics.com/resource/disulfide-bridges-proteins-formation-function-analysis.htm), important for structural stability. Interestingly, ESM started developing this conviction for both positions simultaneously around layer 10.

<Figure content={<Image path={require("./images/antibody-bridge-1.png")} />}>
  PDB [5XRQ](https://www.rcsb.org/3d-sequence/5XRQ?asymId=A) has a disulfide
  bridge across positions 22 and 96.
</Figure>

There is another disulfide bridge spanning positions 154 - 210. ESM seems to have noticed this one starting from layer 9.

<Figure content={<Image path={require("./images/antibody-bridge-2.png")} />}>
  (PDB [5XRQ](https://www.rcsb.org/3d-sequence/5XRQ?asymId=A)) has another
  disulfide bridge across positions 154 and 210.
</Figure>

Here is the structure colored by logits (the other chain is in grey).

<StructureOverlay
  title="Antibody heavy chain (PDB 5XRQ) structure colored by top logit"
  pdbId="5XRQ"
  logitsPath="/data/logit-lens/ab_heavy_chain_top_logits.csv"
  maxLogit="12"
/>

And the true amino acid ranks:

<TrueTokensRanksHeatmap
  title="Antibody heavy chain (PDB 5XRQ) true token ranks"
  sequence="QVQLVQSGAEVKKPGSSVRVSCKASGDTFSSYSITWVRQAPGHGLQWMGGIFPIFGSTNYAQKFDDRLTITTDDSSRTVYMELTSLRLEDTAVYYCARGASKVEPAAPAYSDAFDMWGQGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKRVEPKSCHHHHHH"
  ranksPath="/data/logit-lens/ab_heavy_chain_true_token_ranks.csv"
/>

## Attention maps

We can visualize a Transformer's attention maps to learn about activities of the attention heads at each layer. Can we use this to explain what we saw in the logit lens?

From layer 9, ESM began noticing the disulfide bridge at positions 154 - 210 in the antibody sequence. What are the attention heads doing at that layer? Below are max-pooled attention maps zoomed in at those positions, comparing layer 8 vs. 9.

<Figure
  content={<Image path={require("./images/bridge-attention-maps.png")} />}
/>

At least one of the attention heads in layer 9 is attending to the positions of the disulfide bridge, which doesn't seem to be the case for layer 8. This might explain why ESM started "seeing" the bridge starting at layer 9.

## Final thoughts

We have quite a few tools in our toolbox now for interpreting protein language models: [attention maps](https://arxiv.org/abs/2006.15222), [SAEs](https://www.biorxiv.org/content/10.1101/2024.11.14.623630v1) (plug for [our work](https://www.biorxiv.org/content/10.1101/2025.02.06.636901v1)), and the logit lens. I'm particularly excited about ways we might combine them to gain deeper, systematic understanding of how these models work and answer practical questions:

- Can design better models that more accurately represent biology and avoid common failure modes?
- Assuming protein models encode some knowledge of biology unknown to us, can we use these tools to extract that knowledge?

Compared to LLMs, protein models are hard to interpret because we didn't invent the language of life (and actually barely understand it). But we've got help in some other ways, like <Link to="/protein-evolution">powerful maps of evolution</Link> and beautiful structures. The hidden structures of biological models are quite different––and arguably even more exotic and exhilarating.

## Acknowledgements

Thank you to [Etowah Adams](https://etowahadams.com/) for reading a draft of this post and giving feedback and ideas.
