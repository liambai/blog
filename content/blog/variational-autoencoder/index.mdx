---
title: "A practical introduction to variational autoencoders"
date: "2023-10-14"
description: Predicting protein function using deep generative models. Latent variable models, variational autoencoders (VAEs), Bayesian inference, evidence lower bound (ELBO), mutation effect prediction.
---

import Figure from "../../../src/components/figure.jsx"
import Image from "../../../src/components/image.jsx"
import { Link } from "gatsby"
import MSACoupling from "../protein-evolution/d3/MSACoupling.jsx"
import { Note, NoteList } from "./Notes.jsx"
import DistributionUpdate from "./d3/DistributionUpdate.jsx"
import VariationalInference from "./d3/VariationalInference.jsx"
import Slider from "./d3/ELBOSlider.jsx"

## We are all latent variable models

Here's one way to look at learning. We interact with the world through observing (hearing, seeing) and acting (speaking, doing). We encode our observations about the world into some _representation_ in our brain – and refine it as we observe more. Our actions reflect this representation.

<Figure
  content={<Image path={require("./images/representation.png")} width="50%" />}
></Figure>

### Encoder-decoder

Imitation is an effective way to learn that engages both observation and action. For example, babies repeat the words of their parents. As they make mistakes and get corrected, they hone the way they represent the words they hear (the **encoder**) as well as the way they create their own words from that representation (the **decoder**).

<Figure
  content={
    <Image path={require("./images/encoder-decoder-baby.png")} width="50%" />
  }
></Figure>

Crudely casting this in machine learning terms, the representation is a vector $\mathbf{z}$ called a **latent variable**, which lives in the **latent space**. The baby is a **latent variable model** engaged in a task called **reconstruction**. <Note id={1} />

A note on notation: when talking about distributions, it's usually implied from context whether something is fixed or a variable in a distribution. I find this confusing and want to clarify by making fixed things **bold**. For example, $\mathbf{z} = [0.12, -0.25, -0.05, 0.33, 0.02]$ is a fixed vector, $p(x|\mathbf{z})$ is a conditional distribution over possible values of $x$, i.e. it's a function of $x$.

Given observation $\mathbf{x}$, the encoder is a distribution $q(z|\mathbf{x})$ over the latent space; knowing $\mathbf{x} = \text{``Dog"}$, the encoder tells us which latent variables are probable. To obtain some $\mathbf{z}$, we sample from $q(z|\mathbf{x})$.

Similarly, given some latent variable $\mathbf{z}$, the decoder is a distribution $p(x|\mathbf{z})$. When sampled from, the decoder produces a reconstructed $\mathbf{\tilde{x}}$.

<Figure
  content={
    <Image path={require("./images/encoder-decoder-details.png")} width="50%" />
  }
></Figure>

### Knowing when to say "good job!"

A good model (baby) at reconstruction gets it exactly right with high probability. Given some input $\mathbf{x}$, let's pick some random $\mathbf{z_{rand}}$ and look at $p(\mathbf{x}|\mathbf{z_{rand}})$ – the probability of reconstructing the input perfectly – if it's high, we can tell the model that it did a good job.

But that's not really fair: what if we picked a $\mathbf{z_{rand}}$ that the encoder would never choose? After all, the decoder only sees the latent variables produced by the encoder. Ideally, we want to assign more weight to $\mathbf{z}$'s that the encoder is more likely to produce.

$$
P_{\text{perfect reconstruction}}(\mathbf{x}) = \sum_{\mathbf{z} \in \text{latent space}} q(\mathbf{z}|\mathbf{x}) p(\mathbf{x} | \mathbf{z})
$$

The weighted average is also known as an expectation over $q(z|\mathbf{x})$ <Note id={2}/>:

$$
P_{\text{perfect reconstruction}}(\mathbf{x}) = \mathbb E_{\mathbf{z} \sim q(z|\mathbf{x})}[\log p(\mathbf{x} | \mathbf{z})]
$$

If $P_{\text{perfect reconstruction}}(\mathbf{x})$ is high, we can tell our model that it did a good job.

### But... please don't just memorize

If use use neural networks as both the encoder and the decoder, our model is called a **variational autoencoder (VAE)**.

Neural networks tend to **overfit**. Imagine if our encoder learns to give each input it sees during training its unique corner in the latent space, and the decoder cooperates on this obvious signal.

$$
\mathbf{x} = \text{``Dog"} \xrightarrow{encoder} \mathbf{z} = [1, 0, 0, 0, 0] \xrightarrow{decoder} \mathbf{\tilde{x}} = \text{``Dog"}
$$

$$
\mathbf{x} = \text{``Doggy"} \xrightarrow{encoder} \mathbf{z} = [0, 1, 0, 0, 0] \xrightarrow{decoder} \mathbf{\tilde{x}} = \text{``Doggy"}
$$

We would get perfect reconstruction! But this is not what we want. The model failed to capture the close relationship between "Dog" and "Doggy". A good, generalizable model should treat them similarly by assigning them similar latent variables. In other words, we don't want our model to merely memorize and regurgitate the inputs.

Although a baby's brain is exceptionally good at dealing with this problem, neural networks need a helping hand. One approach is to guide the distribution of the latent variable to be something nice, like a [standard normal](https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution):

$$
p(z) = Normal(0, 1)
$$

We talked previously about [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), a similarity measure between probability distributions: $D_{KL}(q(z | \mathbf{x}) || p(z))$ tells us how far the encoder has strayed from the standard normal. Let's write down the intuition that we want the model to 1) reconstruct well and 2) have an encoder distribution close to the standard normal:

$$
ELBO(\mathbf{x}) = \mathbb E_{\mathbf{z} \sim q(z|\mathbf{x})}[\log p(\mathbf{x} | \mathbf{z})] - D_{KL}(q(\mathbf{z} | \mathbf{x}) || p(\mathbf{z}))
$$

This is the **Evidence Lower BOund (ELBO)**, (we'll explain the name later!), a quantity we want to _maximize_. We can view the KL divergence term as a penalization for complex, nonstandard encoder distributions. This technique to prevent overfitting is called **regularization**.

In machine learning, we're used to _minimizing_. We can define a simple loss function whose minimization is equivalent to maximizing ELBO:

$$
Loss(\mathbf{x}) = - ELBO(\mathbf{x})
$$

Forcing $p(z)$ to be standard normal might seem strange. Don't we want the distribution of $z$ to be something informative learned by the model? Think about it like this: the encoder and decoder are complex functions with many parameters (they're neural networks!) and _they have all the power_. With a sufficiently complex function, $p(z) = Normal(0,1)$ can be transformed into _anything you want_.

<Figure
  content={
    <Image path={require("./images/standard-normal-transformation.png")} />
  }
></Figure>

So far, we talked about variational autoencoders purely through the lens of machine learning. Some of the formulations might feel unnatural, e.g. why do we regularize in this weird way?

Variational autoencoders are actually deeply rooted in a field of statistics called [variational inference](https://en.wikipedia.org/wiki/Variational_Bayesian_methods), which provides the first principles behind our decisions. That is the subject of the next section. However, since we already have a working machine learning model, feel free to [skip it](#modeling-protein-sequences) if you are more interested in how VAEs are applied.

## Why are VAEs the way that they are?

Here's another way to look at the reconstruction task. Every time the baby hears and repeats a word, he updates his internal view of the world. We model this view as a distribution over the latent space, $p(z)$. Learning is nothing but _a series of these updates_.

Given some word $\mathbf{x} = ``Dog"$, the baby performs the update:

$$
p(z) \leftarrow p(z | \mathbf{x})
$$

$p(z)$ is the **prior distribution** (before the update) and $p(z | \mathbf{x})$ is the **posterior distribution** (after the update). This is often called **Bayesian inference** because we use **Bayes rule** to compute the posterior:

$$
p(z | \mathbf{x}) = \frac{p(\mathbf{x} | z) p(z)}{p(\mathbf{x})}
$$

This formula is obvious from a math-symbols perspective <Note id={3}/> but I've always found it hard to understand what it actually means. Let's try to understand it intuitively within our setup.

### Bayesian updates

$p(\mathbf{x})$, called the **evidence**, is the probability of observing $\mathbf{x}$ averaged over all possible latent variables $\mathbf{z}$:

$$
p(\mathbf{x}) = \sum_{\mathbf{z} \in \text{latent space}} p(\mathbf{z})p(\mathbf{x} | \mathbf{z})
$$

When the latent space is massive, as in our case, this quantity is infeasible to compute.

Let's look at Bayes rule purely through the lens of the distribution update: $p(z) \leftarrow p(z | \mathbf{x})$.

1. I have some preconception, $p(z)$
2. I see some $\mathbf{x}$ (e.g. "Dog")
3. Now I have some updated mental model, $p(z | \mathbf{x})$

Concretely, we have a long vector $p(z)$ with a probability value for each possible $\mathbf{z}$ in the latent space. At each update step, we update _every_ value in $p(z)$.

<Figure content={<DistributionUpdate />}></Figure>

How should each new observation $\mathbf{x}$ affect my mental model? At the very least, we should somehow increase $p(\mathbf{x})$, the probability we assign to observing $\mathbf{x}$, since we literally just observed it!

$p(\mathbf{x})$ is an averaged opinion across all the latent variables on how likely $\mathbf{x}$ is. Let's take some random $\mathbf{z}$. Suppose $\mathbf{z}$ leads me to think that $\mathbf{x}$ is likely, say 60%, while the averaged opinion is only 20%. Given that we just observed $\mathbf{x}$, $\mathbf{z}$ outperformed the average. Let's promote it by bumping its assigned probability, proportional to how much it outperformed. Formally, $p(\mathbf{x} | \mathbf{z}) = 0.6$ and $p(\mathbf{x}) = 0.2$:

$$
\frac{p(\mathbf{x}|\mathbf{z})}{p(\mathbf{x})} = \frac{0.6}{0.2} = 3
$$

The posterior we will use to update $p(\mathbf{z})$ is:

$$
p(\mathbf{z} | \mathbf{x}) = 3 * p(\mathbf{z})
$$

This has the effect of 1) assigning a higher probability to a $\mathbf{z}$ that led to an accurate prediction and 2) bringing up $p(\mathbf{x})$, the averaged opinion on how likely $\mathbf{x}$ is.

Conversely, if $\mathbf{z}$ leads me to think that $\mathbf{x}$ is unlikely, say 20%, while the averaged opinion is 60%, then $\mathbf{z}$ underperformed the average. Let's decrease its assigned probability. $p(\mathbf{x} | \mathbf{z}) = 0.2$ and $p(\mathbf{x}) = 0.6$:

$$
\frac{p(\mathbf{x}|\mathbf{z})}{p(\mathbf{x})} = \frac{0.2}{0.6} = 1/3
$$

The posterior we will use to update $p(\mathbf{z})$ is:

$$
p(\mathbf{z} | \mathbf{x}) = 1/3 * p(\mathbf{z})
$$

Similarly, this 1) assigns a lower probability to a $\mathbf{z}$ that led to an inaccurate prediction and 2) brings up $p(\mathbf{x})$, the averaged opinion on how likely $\mathbf{x}$ is.

That's the essence of the update rule: it's controlled by the fraction $\frac{p(\mathbf{x}|\mathbf{z})}{p(\mathbf{x})}$.

### Approximating the posterior

As we mentioned, the evidence $p(\mathbf{x})$ is impossible to compute because it is a sum over all possible latent variables. This means that we can't actually compute the posterior distributions for the update – we need to approximate it.

The two most popular methods for doing this approximation are Markov Chain Monte Carlo (MCMC) and variational inference. We talked about MCMC previously in [various](/protein-hallucination) [contexts](/protein-evolution/#generating-new-sequences). It uses a trial-and-error approach to generate samples from the complex distribution, from which we can approximate the underlying distribution.

Variational inference, in contrast, looks at a collection of distributions. For example, we assume the observations follows a normal distribution and consider all distributions we get by varying the the mean and variance.

<Figure content={<VariationalInference />}></Figure>

Variational inference is a principled way to _vary_ these parameters of the distribution (hence the name!) and find a setting of them that best explains the observations. (N: In the variational autoencoder, these parameters of the parameters of the neural network.)

In our case, we try to use some distribution $q(z | \mathbf{x})$ to approximate $p(z | \mathbf{x})$. We want $q(z | \mathbf{x})$ to be as similar to $p(z | \mathbf{x})$ as possible, which we can enforce by minimizing the KL divergence between them:

$$
D_{KL}(q(z | \mathbf{x}) || p(z | \mathbf{x}))
$$

If the KL divergence is $0$, then $q(z | \mathbf{x})$ perfectly approximates the posterior $p(z | \mathbf{x})$.

### Deriving the Evidence Lower Bound (ELBO)

If you're not interested in the mathematical details, this section can be safely skipped entirely. TLDR: expanding out $D_{KL}(q(z | \mathbf{x}) || p(z | \mathbf{x}))$ yields the equation at the end of the section, the foundational equation of variational inference.

By definition of KL divergence and log rules:

$$
\begin{align*}
D_{KL}(q(z | \mathbf{x}) || p(z | \mathbf{x})) &= \mathbb E_{\mathbf{z} \sim q(z|\mathbf{x})}\left[\log \frac{q(\mathbf{z} | \mathbf{x})}{p(\mathbf{z} | \mathbf{x})}\right]\\
&= \mathbb E_{\mathbf{z} \sim q(z|\mathbf{x})} \left[\log q(\mathbf{z} | \mathbf{x}) - \log p(\mathbf{z} | \mathbf{x}) \right]
\end{align*}
$$

Apply Bayes rule and log rules:

$$
\begin{align*}
D_{KL}(q(z | \mathbf{x}) || p(z | \mathbf{x})) &= \mathbb E_{\mathbf{z} \sim q(z|\mathbf{x})} \left[\log q(\mathbf{z} | \mathbf{x}) - \log \frac{p(\mathbf{x} | \mathbf{z})p(\mathbf{z})}{p(\mathbf{x})} \right] \\
&= \mathbb E_{\mathbf{z} \sim q(z|\mathbf{x})} \left[\log q(\mathbf{z} | \mathbf{x}) - (\log p(\mathbf{x} | \mathbf{z}) + \log p(\mathbf{z}) - \log p(\mathbf{x}))\right] \\
&= \mathbb E_{\mathbf{z} \sim q(z|\mathbf{x})} \left[\log q(\mathbf{z} | \mathbf{x}) - \log p(\mathbf{x} | \mathbf{z}) - \log p(\mathbf{z}) + \log p(\mathbf{x})\right] \\
\end{align*}
$$

Move $\log p(\mathbf{x})$ out of the expectation because it doesn't depend on $\mathbf{z}$:

$$
D_{KL}(q(z | \mathbf{x}) || p(z | \mathbf{x})) = \mathbb E_{\mathbf{z} \sim q(z|\mathbf{x})} \left[\log q(\mathbf{z} | \mathbf{x}) - \log p(\mathbf{x} | \mathbf{z}) - \log p(\mathbf{z})\right] + \log p(\mathbf{x})
$$

Separate terms into 2 expectations and group with log rules:

$$
D_{KL}(q(z | \mathbf{x}) || p(z | \mathbf{x})) = \mathbb E_{\mathbf{z} \sim q(z|\mathbf{x})} \left[ \log \frac{q(\mathbf{z} | \mathbf{x})}{p(\mathbf{z})} \right] - \mathbb E_{\mathbf{z} \sim q(z|\mathbf{x})} \left[\log p(\mathbf{x} | \mathbf{z})\right] +  \log p(\mathbf{x})
$$

The first expectation is a KL divergence: $D_{KL}(q(\mathbf{z} | \mathbf{x}) || p(\mathbf{z}))$. Rewriting and rearranging:

$$
\log p(\mathbf{x}) - D_{KL}(q(z | \mathbf{x}) || p(z | \mathbf{x})) = \mathbb E_{\mathbf{z} \sim q(z|\mathbf{x})} \left[\log p(\mathbf{x} | \mathbf{z})\right] - D_{KL}(q(\mathbf{z} | \mathbf{x}) || p(\mathbf{z}))
$$

This is the central equation in variational inference. The right hand side is exactly what we have called the _evidence lower bound (ELBO)_.

### What does ELBO mean?

From expanding $D_{KL}(q(z | \mathbf{x}) || p(z | \mathbf{x}))$ above, we got:

$$
\log p(\mathbf{x}) - D_{KL}(q(z | \mathbf{x}) || p(z | \mathbf{x})) = ELBO(\mathbf{x})
$$

Now the name makes sense. Since $D_{KL}(q(z | \mathbf{x}) || p(z | \mathbf{x}))$ is always positive (N: a property of KL divergences), $ELBO(\mathbf{x})$ is a _lower bound_ on the (log) evidence, $\log p(\mathbf{x})$.

Our goal of minimizing $D_{KL}(q(z | \mathbf{x}) || p(z | \mathbf{x}))$ is equivalent to maximizing $ELBO(\mathbf{x})$.

<Figure content={<Slider />}></Figure>

## Modeling protein sequences

### Pair-wise models are limiting

In a <Link to="/protein-evolution">previous post</Link>, we talked about ways to extract the information hidden in [Multiple Sequence Alignments (MSAs)](https://en.wikipedia.org/wiki/Multiple_sequence_alignment): the co-evolutionary data of proteins. For example, amino acid positions that co-vary in the MSA tend to be tend to interact with each other in the folded structure, often via direct 3D contact.

<Figure content={<MSACoupling />}>
  An MSA contains different variants of a sequence. The structure sketches how
  the amino acid chain might fold in space (try dragging the nodes). Hover over
  each row in the MSA to see the corresponding amino acid in the folded
  structure. Hover over the blue link to highlight the contacting positions.
</Figure>

We considered models that look at each position (position-wise) and models that additionally consider all possible pairs of positions ([pair-wise](https://en.wikipedia.org/wiki/Potts_model)). But what about the interactions between 3 positions (third-order interactions)? Or even more? Those interactions are commonplace in natural proteins but are unfortunately computationally unfeasible.

### Variational autoencoders for proteins

Let's imagine there being some latent variable vector $\mathbf{z}$ that explains _all_ interactions – including higher-order ones.

<Figure content={<Image path={require("./images/MSA-latent.png")} />}></Figure>

Like the mysterious representation hidden in the baby's brain, we don't need to understand exactly _how_ it encodes these higher-order interactions; we let the neural networks, guided by the reconstruction task, figure it out.

In [this work](https://www.nature.com/articles/s41592-018-0138-4), researchers from the [Marks lab](https://www.deboramarkslab.com/) did exactly this to create a VAE model called [DeepSequence](https://github.com/debbiemarkslab/DeepSequence). The used a neural network with 2 fully connected layers for both the encoder and the decoder.

During training, we feed sequences in the MSA into DeepSequence and use gradient descent to update encoder and decoder parameters (blue arrow). During inference – i.e. when given an unknown input sequence – we pass it through the encoder-decoder in the same way and produce a probability for the input sequence (green arrow).

<Figure
  content={
    <Image
      path={require("./images/DeepSequence-architecture.png")}
      width="90%"
    />
  }
></Figure>

Given an unknown input sequence $\mathbf{x}$, $p(\mathbf{x})$ describes how likely it is for $\mathbf{x}$ to be a functional protein given the evolutionary sequences in our MSA. $p(\mathbf{x})$ is a predictor of fitness with a range of applications, from improving protein function to from interpreting human disease variants.

## Predicting mutation effects

Given a wild-type sequence $\mathbf{x}^{\text{(Wild-type)}}$ and a mutant sequence $\mathbf{x}^{\text{(Mutant)}}$, the log ratio

$$

\log\frac{p(\mathbf{x}^{\text{(Mutant)}})}{p(\mathbf{x}^{\text{(Wild-type)}})}


$$

measures the improvement of the mutant over the wild-type.

- Model assigns $\mathbf{x}^{\text{(Mutant)}}$ a higher probability than $\mathbf{x}^{\text{(Wild-type)}}$ $\rightarrow$ the mutation is likely beneficial $\rightarrow$ positive log ratio.

- Model assigns $\mathbf{x}^{\text{(Mutant)}}$ a lower probability than $\mathbf{x}^{\text{(Wild-type)}}$ $\rightarrow$ the mutation is likely harmful $\rightarrow$ negative log ratio.

### Disease variants

We can compute this log ratio for each possible amino acid substitution at each position to produce a heatmap that illustrates their consequences:

<Figure
  content={<Image path={require("./images/mutation-effect-heatmap.png")} />}
></Figure>

Predicting the pathogenicity of human protein variants like this can be an important guide for clinical decisions. Most of the variants of human proteins related to disease (98% of [genomAD](https://gnomad.broadinstitute.org/)!) have unknown consequences, a testament to <Link to="/protein-representation">the difficulty of obtaining functional data relative to sequence data</Link>. This means:

1. we are deeply ignorant about the proteins in our bodies and how their malfunctions cause disease.

2. unsupervised computational approaches like VAEs can make a huge impact.

[ClinVar](https://www.ncbi.nlm.nih.gov/clinvar/intro/) is a public archive that sheds light in this darkness. It collects experimental data on the impacts of mutations in proteins important to human health. Predictions from a VAE-based approach called [EVE](https://github.com/OATML/EVE) agrees strongly with labels from ClinVar.

<Figure content={<Image path={require("./images/EVE-ClinVar.png")} />}>
  EVE outperforms other computational approaches of variant effect prediction in
  concordance with ClinVar. On the y-axis, [Deep Mutational Scanning
  (DMS)](https://www.nature.com/articles/nmeth.3027) is an experimental method
  for screening a large set of variants for a specific function, another source
  of experimental labels. When evaluated on the concordance with DMS data, EVE
  similarly outperforms others models.
</Figure>

I find the VAE model's strong predictive power incredible. After all, the model is completely unsupervised: it has never seen _any_ labeled data of variant phenotype. It learns purely through studying the evolutionary sequence in the protein's family.

### The power of latent variables

When compared to independent position-wise models, the latent variable model is significantly more accurate for all possible mutations.

<Figure
  content={<Image path={require("./images/DeepSequence-vs-others.png")} />}
></Figure>

The sites at which their accuracy improved the most are ones that cooperate with several other sites.

<Figure
  content={<Image path={require("./images/DeepSequence-vs-independent.png")} />}
></Figure>

This demonstrates the flexibility of the latent variable model that can capture higher-order interactions.

Here's one way to look at these results. MSAs contains a wealth of information, some of which we can understand through simple statistics: position-wise frequencies, pair-wise frequencies, etc. (what we did in a <Link to="/protein-evolution">previous post</Link>). Those models are interpretable but limiting – they fail at teasing out more complex, higher-order signals.

Enter neural networks, which are much better than us at recognizing those signals hidden in MSAs. They known _where to look_, _what to look at_ – beyond our simple statistics. This comes at the cost of interpretability. The comparison between our VAE model and statistical models like <Link to="/protein-evolution">Direct Coupling Analysis (DCA)</Link> exemplifies these two paradigms.

<NoteList />
$$
