---
title: What we can learn from evolving proteins
date: "2023-09-16"
description: Predicting protein structure and function. Multiple Sequence Alignments (MSAs), the Potts model, Direct Coupling Analysis (DCA), EVCouplings.
---

import MSACoupling from "./MSACoupling.jsx"
import Distributions from "./Distributions.jsx"
import MSAHighlighted from "./MSAHighlighted.jsx"
import MSAFrequencies from "./MSAFrequencies.jsx"
import MSACovariance from "./MSACovariance.jsx"
import Image from "../../../src/components/image.jsx"

Proteins are incredible molecular machines that orchestrate almost all activity in our
biological world. We are only beginning to understand them. This article is a deep dive
into some statistical methods that enable us to understand new proteins by harnessing
the power of evolution.

Amino acids make up proteins and specify their structure and function. Over millions of years,
evolution has conducted a massive experiment over the space of all possible amino acid sequences:
those that encode a functional protein survive; those that don't are extinct.

We can learn a surprising lot about a protein by studying similar variants of it
we find in nature (its **protein family**). These hints from evolution have propelled
breakthroughs like AlphaFold and cutting-edge methods in protein design. Let's see how.

A **Multiple Sequence Alignment (MSA)** compiles known variants of a protein and is compiled by
searching vast protein sequence databases.

<MSACoupling
  caption="
    The structure on the right sketches how the amino acid chain might fold in space (try dragging the nodes). 
    Hover over each row in the MSA to see the corresponding amino acid in the
    folded structure. Hover over the blue link to see the contacting amino acid positions. 
  "
/>

One key insight is that amino acid positions that tend to co-vary in the MSA likely
interact with each other in the folded structure, often via direct 3D contact. In the rest of this
article, we'll dissect this MSA and make this idea concrete.

## In search of a distribution

Let's start with the question: given an MSA and an amino acid sequence, what's the probability that the
sequence encodes a functional protein in the family of the MSA? In other words, given a
sequence $A = (A_1, A_2, ..., A_L)$, we're looking for an appropriate probability distribution
$P(A)$ based on the MSA.

Knowing $P$ is powerful. It lends us insight into sequences that we've never
encountered before (more on this later!). Oftentimes, $P$ is called a _model_.
For the outcome of rolling a die, we have great models; proteins,
unfortunately not so much.

<Distributions
  caption="
    Hover over the bars to see the probabilities. Sequence probabilities are made up but follow
    some patterns you might expect: sequences that are in or resemble sequences in the MSA have
    higher probabilities. The set of all possible sequences, called the sequence space, is
    mind-bendingly vast: the number of possible 10 amino acid sequences is 20^10 (~10 trillion)
    because there are 20 amino acids; the bar graph is extremely truncated.
  "
/>

### Counting amino acid frequencies

Let's take a closer look the MSA.

<MSAHighlighted />

Some positions have the same amino acid across almost all rows.
For example, every sequence has A in the first position – it is **evolutionarily conserved** –
which means that it's probably important!

To measure this concretely, let's count the frequencies of observing each amino acid
at each position. Let $f_i(A_i)$ denote the frequency of observing the amino acid $A_i$
at position $i$.

<MSAFrequencies
  caption="
    Hover over the MSA to compute amino acid frequencies.
  "
/>

If we have some sequence $A$ of $L$ amino acids, it's natural to define

$$
E(A) = \sum_{1 \leq i \leq L} f_i(A_i)
$$

$E(A)$ should be big when the amino acid frequencies in each position of $A$ matches
the frequency patterns observed the in MSA, and small otherwise. For example, if $A$
starts with the amino acid L, then $f_1(\text{L}) = 1$ is contributed to the sum; if it starts
with any other amino acid, $0$ is contributed.

$E$ is often called the **energy function** – it's not a probability distribution, but we
can easily turn it into one by normalizing its values to sum to $1$ (let's worry about that
later).

### Pairwise frequencies

But what about the co-variation between pairs of positions? As hinted in the introduction,
it has important implications for the structure (and hence function) of a protein. Let's
also count the co-occurrence frequencies.

Let $f_{ij}(A_i, A_j)$ denote the frequency of observing amino acid $A_i$ at position
$i$ _and_ amino acid $A_j$ at position $j$ at position $i$.

<MSACovariance
  caption="
    Hover over the MSA to compute amino acid pairwise frequencies.
  "
/>

Adding these pairwise terms to our energy function:

$$
E(A) = \sum_{1 \leq i \leq j \leq L} f_{ij} (A_i, A_j)+\sum_{1 \leq i \leq L} f_i(A_i)
$$

Now, we have a simple model that accounts for single-position amino acid frequencies _and_
pairwise co-occurrence frequencies! In practice, the pairwise terms are often a bit more
sophisticated and involve some more calculations based on the co-occurrence frequencies
(we'll walk through how it's done in a popular method called
[EVCouplings](https://evcouplings.org/) soon), but let's take a moment to appreciate this
energy function in this general form.

$$
E(A) = \sum_{1 \leq i \leq j \leq L} J_{i j} (A_i, A_j)+\sum_{1 \leq i \leq L} h_i(A_i)
$$

As it turns out, physicists have thought about this problem since the 1950s, in a different
context: the interacting spins of particles in solids like magnets. The $J_{ij}$ terms
capture the energy cost of particles $i$ and $j$ to be in their respective states: its
magnitude is high if they interact, low of they don't; the $h_i$ terms capture the energy
cost of each particle being in its state.

They call this the [Potts model](https://en.wikipedia.org/wiki/Potts_model), and a fancy name
for the energy function is the Hamiltonian. This fascinating field of physics that uses
these statistical models to explain macroscopic behaviors of matter is called statistical
mechanics.

<Image
  path={require("./potts.png")}
  caption="
    The Potts model on a square lattice. Black and white dots are in different states.
    Figure from https://arxiv.org/abs/1511.03031.
  "
  width={300}
/>

### Global pairwise terms

Earlier, we considered using $f_{ij}$ as the term capturing pairwise interactions.
$f_{ij}$ focuses on what's happening at positions $i$ and $j$ – nothing more. It's
a _local_ model. Perhaps
we can imagine a scenario where positions $i$ and $j$ each independently interact with
position $k$, though they do not directly interact with each other. Due to this
**transitive correlation** between $i$ and $j$, the nearsighted $f_{ij}$ would likely
overestimate the interaction between them.

$$
i \longrightarrow k \longleftarrow j
$$

To disentangle such direct and indirect correlations, we want to have a _global_
model that accounts for _all_ pair correlations. [EVCouplings](https://evcouplings.org/)
is a protein structure and function prediction tool that accomplishes this using
[**mean-field approximation**](https://en.wikipedia.org/wiki/Mean-field_theory)
[2]. The calculations are straightforward:

1. Compute the difference between the pairwise frequencies and the independent frequencies
   and store them in a matrix $C$, called the pair excess matrix

$$
C_{ij}(A_i, A_j) = f_{ij}(A_i, A_j) - f_i(A_i)f_j(A_j)
$$

2. Compute the inverse of this matrix, $C^{-1}$, the entries of which are just the negative
   of the $J_{ij}$ terms we seek

$$
J_{ij}(A_i, A_j) = - (C^{-1})_{ij}(A_i, A_j)
$$

The theory behind these steps is involved and beyond our scope, but intuitively, we can
think of the matrix inversion as an averaging step that accounts for $all$ pair correlations
and makes the model global. This method is often referred to as
[**Direct Coupling Analysis**](https://en.wikipedia.org/wiki/Direct_coupling_analysis)
(DCA).

### The distribution

We can turn our energy function into a probability distribution by 1) exponentiating, creating
an [exponential family distribution](https://en.wikipedia.org/wiki/Exponential_family)
that is mathematically easy to work with, and 2) dividing by the appropriate normalization
constant $Z$ to make all probabilities sum to 1.

$$
P(A)=\frac{1}{Z} \exp \left\{\sum_{1 \leq i \leq j \leq L} J_{i j}(A_i, A_j)+\sum_{1 \leq i \leq L} h_i(A_i)\right\}
$$

## Predicting 3D structure

Given an amino acid sequence, what is the 3D structure that it folds into? This is the
[protein folding problem](https://rootsofprogress.org/alphafold-protein-folding-explainer), a
complex problem central to understanding biology. In 2021, researchers from DeepMind shared a
groundbreaking model using deep learning, [AlphaFold](https://www.nature.com/articles/s41586-021-03819-2),
declaring the problem as solved. The
[implications](https://moalquraishi.wordpress.com/2020/12/08/alphafold2-casp14-it-feels-like-ones-child-has-left-home/)
are profound.

The [EVCouplings](https://evcouplings.org/) method we'll describe cannot compete with AlphaFold in
accuracy, but it is foundational to AlphaFold, which similarly relies heavily on pairwise interaction
signals from MSAs.

Myriad forces choreograph the folding of a protein. Let's simplify and think only about
pairs of amino acid positions that interact strongly with each other – and hypothesize that
they are in spatial contact. These predicted contacts can act as a set of constraints from
which we can then derive the full 3D structure.

<MSACoupling
  caption="
    The structure on the right sketches how the amino acid chain might fold in space (try dragging the nodes). 
    Hover over each row in the MSA to see the corresponding amino acid in the
    folded structure. Hover over the blue link to see the contacting amino acid positions. 
  "
/>

Hovering over the blue link, we see that positions $2$ and $8$ tend to co-vary in the MSA,
and they are in contact in the folded protein. Let's use the tools we have to quantify this
co-variance.

### Mutual information

Our $f_{ij}$ is a function that takes in two amino acids: $f_{ij}(A_i, A_j)$; however, we would like
a direct measure of interaction given only positions $i$ and $j$, without a dependence on specific
amino acids. In other words, we want to average over all possible pairs of amino acids that can
inhabit the two positions $i$ and $j$. To do this in a principled and effective way, we use a concept
called **mutual information**:

$$
MI_{i j}=\sum_{A_i, A_j \in \mathcal X} f_{i j}\left(A_i, A_j\right) \ln \left(\frac{f_{i j}\left(A_i, A_j\right)}{f_i\left(A_i\right) f_j\left(A_j\right)}\right)
$$

where $\mathcal X$ is the set of 20 possible amino acids.

Mutual information quantifies the amount of [information](https://en.wikipedia.org/wiki/Information_content)
shared by $i$ and $j$: how much information we gain about $j$ by observing $i$. When $MI_{ij}$ is big,
$i$ and $j$ are highly correlated and are therefore more likely to be in contact. This concept comes
from a beautiful branch of mathematics called
[information theory](https://en.wikipedia.org/wiki/Information_theory),
initially developed by
[Claude Shannon](https://www.quantamagazine.org/how-claude-shannons-information-theory-invented-the-future-20201222/)
at Bell Labs in application to signal transmission in telephone systems.

### Direct information

As we talked about, the local nature of $f_{ij}$ can be limiting: for one, it's bad at
discerning transitive correlations. [EVCouplings](https://evcouplings.org/) uses a different
quantity to approximate the probability that $i$ and $j$ are in contact:

$$
P_{i j}^{D i r}\left(A_i, A_j\right)=\frac{1}{Z} \exp \left\{J_{i j}\left(A_i, A_j\right)+\tilde{h}_i\left(A_i\right)+\tilde{h}_j\left(A_j\right)\right\}
$$

where the $J_{ij}$'s are the global interaction terms obtained by mean-field approximation, and the
$\tilde{h}$ terms can be calculated by imposing the following constraints

$$
\sum_{A_j \in \mathcal X}P_{i j}^{D i r}\left(A_i, A_j\right) = f_i(A_i)
$$

$$
\sum_{A_i \in \mathcal X}P_{i j}^{D i r}\left(A_i, A_j\right) = f_j(A_j)
$$

These constraints ensure that $P_{i j}^{D i r}$ follows the single amino acid frequencies we observe.
For each pair of positions:

1. Let's fix the amino acid at position $i$ to be L. Consider $P_{i j}^{D i r}(L, \mathrm{A_j})$
   for all possible $A_j$'s if we sum them all up, we get the probability of observing $L$
   independently at position $i$, which should be $f_i(L)$.

2. The same idea but summing over all $A_i$'s.

Once we have $P_{i j}^{D i r}$, we can average over all possible $A_i$'s and $A_j$'s like we did for mutual information:

$$
DI_{i j}=\sum_{A_i, A_j \in \mathcal X} P_{i j}^{\text {Dir }}\left(A_i, A_j\right) \ln \left(\frac{P_{i j}^{\text {Dir }}\left(A_i, A_j\right)}{f_i\left(A_i\right) f_j\left(A_j\right)}\right)
$$

This measure is called **direct information**, a more globally-aware measure of pairwise interactions.
When compared to real contacts in experimentally determined structures, DI performed much better than MI,
demonstrating the usefulness of considering the global sequence context [1].

<Image
  path={require("./DI-vs-MI.png")}
  caption="
  Axes are amino acid positions.
  The grey grooves are the actual contact in the experimentally obtained structures.
  The red dots are the predicted contacts using DI; the blue dots are the predicted
  contacts using MI. Data is shown for 2 proteins: ELAV4 and RAS. Figure from [1].
  "
/>

### Constructing the structure

Given predicted contacts by DI, we need to carry out a few more computational steps
– e.g. [simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing) – to
generate the full predicted 3D structure. Omitting those details: the results are
these beautiful predicted structures that often closely match the real structures.

<Image
  path={require("./structures.png")}
  caption="
  Grey structures are real, experimentally observed; red structures are predicted using DI.
  Root mean square deviation (RMSD) measures the average distance between atoms in the predicted
  vs. observed structure and is used to score the quality of structure predictions; they are
  shown on the arrows with the total number of amino acid positions in parentheses.
  Figure from [1].
  "
/>

## Predicting function

At this point, you might think: this is all neat and all, but is it directly useful in
any way? One common problem in industrial biotechnology is: given a protein that carries
out some useful function – e.g. an enzyme that catalyses a desired reaction – how can we
improve it by increasing its stability or activity?

One approach is [saturation mutagenesis](https://en.wikipedia.org/wiki/Saturation_mutagenesis):
take the protein's sequence, mutation every position to every possible amino acid, and test
all the mutants to see if any yields an improvement. I know that sounds crazy, but it has been
made possible by impressive advancements in automation-enabled
[high-throughput screening](https://en.wikipedia.org/wiki/High-throughput_screening) (in
comparison, progress in our biological understanding necessary to make more informed designs
has generally lagged behind). Can we do better?

### Predicting mutation effects

Remember our energy function that measures the fitness of a sequence in the context of an MSA.

$$
E(A) = \sum_{1 \leq i \leq j \leq L} J_{i j} (A_i, A_j)+\sum_{1 \leq i \leq L} h_i(A_i)
$$

Let's try to use this energy function as a guide to our experimental testing. Intuitively,
sequences with low energy should be more likely to fail. Let $A^{\mathrm{wt}}$denote a wildtype,
or natural, sequence, and let $A^{\mathrm{mut}}$ denote a mutant sequence:

$$
\Delta E\left(A^{\mathrm{mut}}, A^{\mathrm{wt}}\right)=E\left(A^{\mathrm{mut}}\right)-E\left(A^{\mathrm{wt}}\right)
$$

captures how much the mutant's energy improved over the wildtype.

In this [paper](https://www.nature.com/articles/nbt.3769) introducing the
[mutation effect prediction tool](https://marks.hms.harvard.edu/evmutation/)
in EVCouplings, researchers computed the $\Delta E$ of each mutant sequence in a saturation
mutagenesis experiment on a protein called M.HaeIII.

<Image
  path={require("./deltaE-mutations.png")}
  caption="Deeper shades of blue reflect more negative ΔE. Most mutations are damaging. Averages across amino
  acids are shown as a bar on the bottom, labeled with * (sensitivity per site). Figure from [3]."
  width="100%"
/>
<br />

Not all positions are created equal – mutations at some positions are especially harmful. Moreover,
the big swathes of blue (damaging mutations) speak to the difficulty of engineering proteins.

The calculated energies correlated strongly with experimentally observed fitness (!), meaning
that our energy function provides helpful guidance on how a given mutation might affect
function. It's remarkable that with such a simple model and seemly so little information (just
from MSAs), we can attain such profound predictive power.

<Image
  path={require("./deltaE-experimental.png")}
  caption="
  Evolutionary statistical energy refers to our energy function E.
  Left plot shows all mutants; right plot shows averages over amino acids at each position.
  Figure from [3].
  "
  width={450}
/>
<br />

The next time we find ourselves trying a saturation mutagenesis screen to identify an improved
mutant, we can calculate some $\Delta E$'s first before stepping in the lab and focus only on
the sequences that have more positive $\Delta E$'s and therefore more likely to succeed.

### Generating new sequences

## Links

The ideas we discussed are primarily from the following papers:

- [Protein 3D structure computed from evolutionary sequence variation](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0028766#pone.0028766.s017).
  This paper describes DCA in detail along with useful intuitions. It's highly accessible and worthwhile.

- [Mutation effects predicted from sequence co-variation](https://www.nature.com/articles/nbt.3769).
  This paper presents the results on predicting mutation effects and introduces the powerful
  [EVMutation](https://marks.hms.harvard.edu/evmutation/).

### References

[1] Marks, D.S. et al.
[Protein 3D structure computed from evolutionary sequence variation](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0028766#pone.0028766.s017).
PLoS One 6, e28766 (2011).

[2] Weigt, M et all.
[Direct-coupling analysis of residue coevolution captures native contacts across many protein families](https://www.pnas.org/doi/10.1073/pnas.1111471108).
Proc. Natl. Acad. Sci. U.S.A. 108, E1293–E1301 (2011).

[3] Hopf, T. et al. [Mutation effects predicted from sequence co-variation](https://www.nature.com/articles/nbt.3769).
Nat Biotechnol 35, 128–135 (2017).

[4] Russ WP, et al. [An evolution-based model for designing chorismate mutase enzymes](https://www.science.org/doi/10.1126/science.aba3304).
Science. 2020;369(6502):440–445 (2020).
