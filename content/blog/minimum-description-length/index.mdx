---
title: "Why say lot word when few do trick?"
date: "2025-06-05"
description: "The Minimum Description Length (MDL) principle, Kolmogorov complexity, curve fitting, learning as data compression."
---

import { Link } from "gatsby"
import Figure from "../../../src/components/figure.jsx"
import Image from "../../../src/components/image.jsx"

It's almost impossible to watch this Kevin clip without coming to the conclusion: he's onto something.

<Figure content={<Image path={require("./images/kevin.jpg")} />} />

His abbreviated sentences do seem to convey the same information as their verbose original––they certainly do trick!

It's natural to ask: can we rigorously define and measure the _information_ in a sentence? How "few" can we go without losing information?

The answer is arguably the most profound idea in machine learning: the **Minimum Description Length (MDL) Principle**. It's so important that when [Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever) gave [John Carmack](https://en.wikipedia.org/wiki/John_Carmack) a list of [30 papers](https://github.com/dzyim/ilya-sutskever-recommended-reading) and said:

> If you really learn all of these, you'll know 90% of what matters today.

he included 4 on this topic.

It's one of those ideas that gave me a different way to see many familiar things. I hope to convince you of the same!

## Strings

How _complex_ are the following binary strings?

1. $00000000000000000000$
2. $10001000100010001000$
3. $01110100110100100110$

Intuitively, the first one is dead simple: just a bunch of zeros. The second, with $1000$ on repeat, is a bit more complex. The third, with no discernable pattern, is the most complex.

Here's one way to define complexity, called **Kolmogorov complexity**: the length of a string is the _shortest computer program_ that outputs it. In what programming language? As it turns out, it doesn't really matter. Let's illustrate with Python:

1. To get $00000000000000000000$, we'd write:

```python
def f():
    return "0" * 20
```

2. To get $10001000100010001000$, we need to type a bit more:

```python
def f():
    return "1000" * 5
```

3. To get $01110100110100100110$, we have to return the whole string as-is:

```python
def f():
    return "01110100110100100110"
```

Making this mathematically rigorous takes some work – we need to define a programming language, measure the length of programs in bits, etc. – and in all but the simplest cases, spelling out the program like we did is impractical. But that's the basic idea: an object is complex if we need a long Python function to return it. This function is called a **description** of the string. Assuming we've found the shortest, its length is the string's **minimum description length**.

What separates these strings from each other? For 1 and 2, we can exploit their regularity (the repeating pattern) to represent them in a more compact way; for 3, we can't.

Another way to think about this is in terms of **data compression**. A string is complex if it is hard to compress.

## Points

What is the Kolmogorov complexity of these 10 points?

<Figure content={<Image path={require("./images/points.png")} width="60%" />} />

Let's try to find the minimum description. Of course, we can just describe each point by its coordinate, but can we do better?

Let's draw a line through the points and use it do describe each point. Here are 3 attempts:

<Figure content={<Image path={require("./images/polynomials.png")} />} />

My knee-jerk reaction to these lines is: left & right bad, middle good! Under/over-fitting, unseen data, bias/variance tradeoff... you name it. But beyond these statistical arguments, here's another way to see why the middle one is best: it offers the _shortest description_ of these points.

The descriptions would look something like:

1. I have a line $y = 0.58x -0.12$ and it misses the first point by $0.21$, the second by $0.13$...
2. I have a line $y = 5.45x^3 - 5.68x^2 + 1.19x + 0.06$ and it misses the first point by $0.03$, the second by $-0.05$...
3. I have a line $y = -15348.64x^9 + 67461.06x^8 - 123937.33x^7 + ...$ and it fits each point perfectly.

In the first case, it's easy to describe the line, but it'll take some effort to describe how far each point is from the line, the errors. The third line is very complicated to describe, but we waste no time talking about the errors. The middle line balances the two.

More generally, we call the line a **hypothesis** $H$, drawn from a set $\mathcal{H}$ of hypotheses, e.g. all polynomials. There is a tradeoff between the description length $L$ of the hypothesis (the coefficients), and the description length of the data $D$ when encoded with the help of the hypothesis (the errors). We want to minimize the sum of these 2 terms:

$$
L(D)=\min _{H \in \mathcal{H}}(L(H)+L(D|H))
$$

This is called the **two-part MDL code**.

The descriptions above are all hand-wavy. How do we quantify the intuition that the errors of the degree 1 polynomial are "harder to explain" than the errors of the degree 3 polynomial? The next section uses tools from information theory to make this calculation precise. Feel free to skip it – here's the spoiler: the degree 3 polynomial gives the optimal two-part MDL code.

### Coding in bits

The calculation we need to compare the 3 descriptions essentially boils down to encoding real numbers – coefficients, errors – as bits. Real numbers have an infinite decimal expansion, so we must approximate to some finite decimal point.

Here's a naive way to do it: type in number in Python. The [float](https://en.wikipedia.org/wiki/Floating-point_arithmetic) type uses 64 bits to represent the number. That's quite wasteful! Going after the theoretical limit of "what's the shortest possible code for these numbers?", we need a better system.

Floating point numbers represent `0`, `0.1`, and `1.7976931348623157e+308` (the largest possible representation) using the same number of bits. But we know that the coefficients and errors come from some distribution (let's assume [Normal](https://en.wikipedia.org/wiki/Normal_distribution)), and we're far more likely to see `0` and `0.1` than `1.7976931348623157e+308`.

One idea from information theory is to use a shorter code for the more likely outcomes like `0` and `0.1`, and a longer code for the those rare events like `1.7976931348623157e+308`. Claude Shannon even proved that the optimal code length is $-\log_2(p(x))$, where $p(x)$ is the probability of event $x$.

<Figure
  content={
    <Image path={require("./images/optimal-code-length.png")} width="60%" />
  }
/>

For example, if you're think a number comes up as often as 50% of the time, you should represent it with only 1 bit.

Assuming the coefficients and errors follow a Normal distribution with mean $0$, we can chop up the real number line into small intervals of size $t$ and assigned each interval with a probability $p(x)$.

<Figure
  content={
    <Image
      path={require("./images/hinton-gaussian-interval.png")}
      width="60%"
    />
  }
/>

### Coding the coefficients

We can now encode each of our polynomial coefficients $w_i$ using its discretized probability:

$$
p(w_i) = t \frac{1}{\sqrt{2 \pi} \sigma_w} \exp \left(\frac{-w_i^2}{2 \sigma_w^2}\right)
$$

where $\sigma_w$ is some fixed value we choose. Calculating the optimal code length, in $\log_2 (e)$ bits (or _nats_):

$$
-\log p(w_i) = -\log t + \log \sqrt{2 \pi} + \log \sigma_w + \frac{w_i^2}{2 \sigma_w^2}
$$

Summing over all weights $w_1, ..., w_n$ to get the full code length of the polynomial:

$$
\begin{align*}
L(H) &= \sum_{i=1}^n -\log p(w_i) \\
&= \sum_{i=1}^n -\log t + \log \sqrt{2 \pi} + \log \sigma_w + \frac{w_i^2}{2 \sigma_w^2} \\
&= \underbrace{n (-\log t + \log \sqrt{2 \pi} + \log \sigma_w)}_{\text{constant}} + \frac{1}{{2 \sigma_w^2}} \sum_{i=1}^n w_i^2 \\
\end{align*}
$$

We see that minimizing the code length of the polynomial is equivalent to minimizing the term $\sum_{i=1}^n w_i^2$, i.e. we want to keep the coefficients small.

### Coding the errors

Applying the same technique to each error term $|d_c - y_c|$, where $d_c$ is the true data point and $y_c$ is our polynomial's approximation:

$$
p(d_c - y_c) = t \frac{1}{\sqrt{2 \pi} \sigma_d} \exp \left(\frac{-(d_c - y_c)^2}{2 \sigma_d^2}\right)
$$

Here, $\sigma_d$ should be optimally set to the standard deviation of the points. Computing the full code length over the 10 data points:

$$
\begin{align*}
L(D|H) &= \sum_{c=1}^{10} -\log p(d_c - y_c) \\
&= \underbrace{10 (-\log t + \log \sqrt{2 \pi} + \log \sigma_d)}_{\text{constant}} + \frac{1}{{2 \sigma_d^2}} \sum_{c=1}^{10} (d_c - y_c)^2 \\
\end{align*}
$$

Minimizing the code length of the errors is equivalent to minimizing $\sum_{c=1}^{10} \frac{(d_c - y_c)^2}{2 \sigma_d^2}$, i.e. we want the errors to be small.

### Regression

Adding the two terms together, we get a minimization objective $C(D)$ equivalent to minimizing the MDL code:

$$
C(D) = \underbrace{\frac{1}{{2 \sigma_d^2}} \sum_{c=1}^{10} (d_c - y_c)^2}_{MSE} + \underbrace{\frac{1}{{2 \sigma_w^2}} \sum_{i=1}^n w_i^2}_{regularization}
$$

This fits our intuition that we want to have small coefficients that minimize the errors – the degree 3 polynomial is best!

This formula is also the minimization objective of [Ridge Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). We never explicitly thought about [mean-squared error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) or [regularization](<https://en.wikipedia.org/wiki/Regularization_(mathematics)>): they fell out of our quest for the shortest description of our data.

Under this interpretation, we can think $\sigma_w$ as a hyperparameters of the model that lets us tweak the regularization strength. In the MDL view, its just the width of the Gaussian we used to discretize our coefficients. A small $\sigma_w$ implies a narrow coefficient distribution and, in turn, a strong regularization.

## Words
