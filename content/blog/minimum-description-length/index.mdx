---
title: "Why say lot word when few do trick?"
date: "2025-06-05"
description: "The Minimum Description Length (MDL) principle, Kolmogorov complexity, curve fitting, learning as data compression."
---

import { Link } from "gatsby"
import Figure from "../../../src/components/figure.jsx"
import Image from "../../../src/components/image.jsx"
import { Reference, ReferenceList } from "./References.jsx"

It's almost impossible to watch this [Kevin clip](https://youtu.be/bctjSvn-OC8?si=Jf1Os9V04MIotRdq&t=48) without coming to the conclusion: he's onto something.

<Figure content={<Image path={require("./images/kevin.jpg")} />} />

His abbreviated sentences do seem to convey the same information as their verbose original. They certainly do trick!

Can we rigorously define and measure the information in a sentence? How "few" can we go without losing information?

These questions lead to one of the most profound ideas in machine learning: the **Minimum Description Length (MDL) Principle**. It's so important that when [Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever) gave [John Carmack](https://en.wikipedia.org/wiki/John_Carmack) a list of [30 papers](https://github.com/dzyim/ilya-sutskever-recommended-reading) and said:

> If you really learn all of these, you'll know 90% of what matters today.

4 on them were on this topic <Reference id={1} /> <Reference id={2} /> <Reference id={3} /> <Reference id={4} />.

The MLD principle is one of those ideas that blew my mind and fundamentally changed the way I see the world. It's a new perspective on familiar concepts like learning, information, and complexity. Let me show you!

## Strings

Let's start with this question: how _complex_ are these binary strings?

1. $00000000000000000000$
2. $10001000100010001000$
3. $01110100110100100110$

Intuitively, the first one is dead simple: just a bunch of zeros. The second is a bit more complex. The third, with no discernable pattern, is the most complex.

Here's one way to define complexity, called **Kolmogorov complexity**: the length of a string is the _shortest program_ in some programming language that outputs it. Let's illustrate with Python:

1. To get $00000000000000000000$, we'd write:

```python
def f():
    return "0" * 20
```

2. To get $10001000100010001000$, we need to type a bit more:

```python
def f():
    return "1000" * 5
```

3. To get $01110100110100100110$, we have to type out the whole string:

```python
def f():
    return "01110100110100100110"
```

Making this mathematically precise takes some work: we need to define the language, measure the length of programs in bits, etc. But that's the basic idea: an object is complex if we need a long Python function to return it. This function is often called a **description** of the string, and Kolmogorov complexity the **minimum description length**.

What separates these strings from each other? For 1 and 2, we can exploit their repeating pattern to represent them in a more compact way; such a regularity does not exist in 3. Generally, we can think about complexity in terms of **data compression**:

- A string is complex if it is hard to compress.
- Given a string, the most optimal compression algorithm gives us its minimum description, whose length is its Kolmogorov complexity.

Here's a claim that I'll back up through the rest of this post: compression is actually the same thing as _learning_. In this example, we have learned the essence of the first string by writing it as `"0" * 20`. Having to spell out the third string verbatim means that we haven't learned anything about it.

## Points

What is the Kolmogorov complexity of these 10 points?

<Figure content={<Image path={require("./images/points.png")} width="60%" />} />

That's equivalent to asking: what's the minimum description length of these points? Of course, we can just describe each point by its coordinate, but can we do better?

Here's an idea: let's draw a line through the points and use it do describe each point. That means describing 2 things: the line + how the far each point is from the line.

Here are 3 attempts:

<Figure content={<Image path={require("./images/polynomials.png")} />} />

As a stats major, my knee-jerk reaction to these lines is: left & right bad, middle good! Under/over-fitting, unseen data, bias/variance tradeoff... you name it. But there's another way to see why the middle one is best: it gives the _shortest description_ of these points.

The descriptions would look something like:

1. I have a line $y = 0.58x -0.12$ and it misses the first point by $0.21$, the second by $0.13$...
2. I have a line $y = 5.45x^3 - 5.68x^2 + 1.19x + 0.06$ and it misses the first point by $0.03$, the second by $-0.05$...
3. I have a line $y = -15348.64x^9 + 67461.06x^8 - 123937.33x^7 + ...$ and it fits each point perfectly.

In the first case, it's easy to describe the line, but it'll take some effort to describe how far each point is from the line, the errors. The third line is very complicated to describe, but we waste no time talking about the errors. The middle one strikes a balance.

More generally, we call the line a **hypothesis** $H$, drawn from a set $\mathcal{H}$ of hypotheses, e.g. all polynomials. There is a tradeoff between the description length $L$ of the hypothesis (the coefficients), and the description length of the data $D$ when encoded with the help of the hypothesis (the errors). We want to minimize the sum of these 2 terms:

$$
L(D)=\min _{H \in \mathcal{H}}(\underbrace{L(H)}_{\text{length of coefficients}}+\underbrace{L(D|H)}_{\text{length of errors}})
$$

That was all quite hand-wavy. How can we quantify the intuition that the errors of the 1st degree polynomial are "harder to describe" than the errors of the 3rd degree polynomial? The next section uses tools from information theory to make these calculations precise.

### Coding in bits

Concretely defining description length essentially boils down to encoding real numbers – coefficients, errors – in bits. Since real numbers have an infinite decimal expansion, so we must approximate to some finite decimal point.

Here's a naive way to do it: type the number in Python. The [float](https://en.wikipedia.org/wiki/Floating-point_arithmetic) type uses 64 bits to represent every number. It represents `0`, `0.1`, and `1.7976931348623e+308` (the largest possible representation) using the same number of bits. That's quite wasteful. Since we're after the theoretical minimum number of bits, we want to encode each number in as few bits as possible.

In reality, we're far more likely to see `0` and `0.1` than `1.7976931348623e+308` (assuming the coefficients and errors come from, say, a [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)). What if we use a shorter code for the more likely numbers like `0` and `0.1`, and a longer code for the those rare events like `1.7976931348623e+308`. Theoretically, the [optimal code length](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem) is $-\log_2(p(x))$, where $p(x)$ is the probability of event $x$.

<Figure
  content={
    <Image path={require("./images/optimal-code-length.png")} width="60%" />
  }
/>

For example, if a number comes up as often as 50% of the time, you should represent it with only 1 bit.

Assuming the coefficients and errors follow a Normal distribution with mean $0$, we can chop up the real number line into small intervals of size $t$ and assigned each interval with a probability $p(x)$.

<Figure
  content={
    <Image
      path={require("./images/hinton-gaussian-interval.png")}
      width="60%"
    />
  }
>
  Given any real number $v$, we can discretize it by taking a small interval of
  size $t$ around it. Integrating across that slice of the Gaussian distribution
  gives us $v$'s assigned probability. This picture is from sections 3 and 4 of{" "}
  <Reference id={3} />, which also contain a detailed explanation of this
  method.
</Figure>

### Coding the coefficients

We can now encode each of our polynomial coefficients $w_i$ using its discretized probability:

$$
p(w_i) = t \frac{1}{\sqrt{2 \pi} \sigma_w} \exp \left(\frac{-w_i^2}{2 \sigma_w^2}\right)
$$

where $\sigma_w$ is some fixed value we choose. Calculating the optimal code length, in $\log_2 (e)$ bits (or _nats_):

$$
-\log p(w_i) = -\log t + \log \sqrt{2 \pi} + \log \sigma_w + \frac{w_i^2}{2 \sigma_w^2}
$$

Summing over all coefficients $w_1, ..., w_n$ to get the code length of the polynomial:

$$
\begin{align*}
L(H) &= \sum_{i=1}^n -\log p(w_i) \\
&= \sum_{i=1}^n -\log t + \log \sqrt{2 \pi} + \log \sigma_w + \frac{w_i^2}{2 \sigma_w^2} \\
&= \underbrace{n (-\log t + \log \sqrt{2 \pi} + \log \sigma_w)}_{\text{constant}} + \frac{1}{{2 \sigma_w^2}} \sum_{i=1}^n w_i^2 \\
\end{align*}
$$

We see that minimizing the code length of the polynomial is equivalent to minimizing the term $\sum_{i=1}^n w_i^2$, i.e. we want to keep the coefficients small.

### Coding the errors

Applying the same technique to each error term $|d_c - y_c|$, where $d_c$ is the true data point and $y_c$ is our polynomial's approximation:

$$
p(d_c - y_c) = t \frac{1}{\sqrt{2 \pi} \sigma_d} \exp \left(\frac{-(d_c - y_c)^2}{2 \sigma_d^2}\right)
$$

Here, $\sigma_d$ should be optimally set to the standard deviation of the points. Computing the full code length over the 10 data points:

$$
\begin{align*}
L(D|H) &= \sum_{c=1}^{10} -\log p(d_c - y_c) \\
&= \underbrace{10 (-\log t + \log \sqrt{2 \pi} + \log \sigma_d)}_{\text{constant}} + \frac{1}{{2 \sigma_d^2}} \sum_{c=1}^{10} (d_c - y_c)^2 \\
\end{align*}
$$

Minimizing the code length of the errors is equivalent to minimizing $\sum_{c=1}^{10} (d_c - y_c)^2$, i.e. we want the errors to be small.

### Regression & Learning

Adding the two terms together, we get a minimization objective $C(D)$ equivalent to minimizing the description length $L(D)$:

$$
C(D) = \underbrace{\frac{1}{{2 \sigma_d^2}} \sum_{c=1}^{10} (d_c - y_c)^2}_{MSE} + \underbrace{\frac{1}{{2 \sigma_w^2}} \sum_{i=1}^n w_i^2}_{regularization}
$$

This fits our intuition that we want to have small coefficients that minimize the errors – the degree 3 polynomial is best!

This formula is also the minimization objective of [Ridge Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). We never explicitly thought about [mean-squared error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) or [regularization](<https://en.wikipedia.org/wiki/Regularization_(mathematics)>): they fell out of our quest for the shortest description of our data.

Under this interpretation, $\sigma_w$ is a hyperparameters of the model that lets us tweak the regularization strength. In the MDL view, its just the width of the Gaussian we used to discretize our coefficients. A small $\sigma_w$ implies a narrow coefficient distribution and, in turn, a strong regularization.

Back to the claim on compression being the same as learning: perhaps you can agree that these two summaries of this example are equivalent:

1. We have _compressed_ these points using a 3rd degree polynomial, which lets us describe them in very few bits.
2. We have _learned_ a good model of these points, a 3rd degree polynomial, which approximates the underlying distribution.

## Words

Can we apply this MDL perspective to understand LLMs? This section will be some of my high-level, imprecise thoughts.

What is the Kolmogorov complexity of all human knowledge?

## Final thoughts

## References

<ReferenceList />
