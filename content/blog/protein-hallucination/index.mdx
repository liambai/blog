---
title: Protein Inception
date: "2023-10-08"
description: Protein design. DeepDrem, hallucinations, Markov Chain Monte Carlo (MCMC), ...
---

import { Link } from "gatsby"
import Figure from "../../../src/components/figure.jsx"
import Image from "../../../src/components/image.jsx"
import { Note, NoteList } from "./Notes.jsx"
import { Reference, ReferenceList } from "./References.jsx"

Models that are good at making predictions also possess some generative power. We saw this theme play out in <Link to="/protein-evolution">previous</Link> <Link to="/protein-representation">posts</Link> with a technique called **Markov Chain Monte Carlo (MCMC)**. Here's a quick recap:

Imagine you have a monkey that, when shown an image, always gets visibly excited if the image contains bananas – and sad otherwise.

<Figure content={<Image path={require("./images/monkey-model.png")} />} />

An obvious task the monkey can help with is image classification, discriminating images with bananas and ones without. The monkey is a **discriminative model**.

Now suppose you want to create some _new_ images of bananas. We can start with a white-noise image:

<Figure
  content={
    <Image
      path={require("./images/white-noise.png")}
      width="50%"
      mobileWidth="60%"
    />
  }
/>

randomly change a couple pixels, and show it to our monkey:

- If he gets more excited, then we've probably done something that made the image more banana-like. Great – let's keep the changes.
- If he doesn't doesn't get more excited – or god forbid, gets less excited – let's discard the changes <Note id={1}/>.

Repeat this thousands of times: we'll end up with an image that looks a lot like bananas! This is the essence of MCMC, which turns our monkey into a **generative model**.

Researchers at Google used a similar technique in a cool project called [DeepDream](https://en.wikipedia.org/wiki/DeepDream). Instead of monkeys, they used [**convolutional neural networks (CNNs)**](https://en.wikipedia.org/wiki/Convolutional_neural_network).

<Figure content={<Image path={require("./images/deepdream-bananas.png")} />}>
  "Optimize with prior" refers to the fact that to make this work well, we
  usually need to constrain our generated images to have some features of
  natural images: for example, neighboring pixels should be correlated. Figure
  from the [blog
  post](https://blog.research.google/2015/06/inceptionism-going-deeper-into-neural.html)
  on DeepDream.
</Figure>

The resulting images have a dream-like quality and are often called **hallucinations**.

Let's replace the banana recognition task with one we're not so good at: predicting the fitness of proteins – and creating new ones with desired properties. The ability to do this is revolutionary to industrial biotechnology and therapeutics. In this post, we'll explore how approaches similar to DeepDream can be used to design new proteins.

## The model: trRosetta

### Overview

**transform-restrained Rosetta (trRosetta)** is a structure prediction model that, like pretty much everything we'll talk about in this post, was developed at the [Baker lab](https://www.bakerlab.org/). trRosetta has 2 steps:

1. Given a [Multiple Sequence Alignment (MSA)](https://en.wikipedia.org/wiki/Multiple_sequence_alignment), use a CNN to predict 6 structure-defining numbers _for each pair of residues_ <Note id={2}/>.

2. Use the 6 numbers produced by the CNN as input to the [Rosetta](https://www.rosettacommons.org/software) structure modeling software to generate 3D structures.

Let's focus on step 1. One structure-defining number produced by trRosetta is the distance between the residues, $d$. There's also this angle $\omega$:

<Figure
  content={
    <Image
      path={require("./images/interresidue-distance.png")}
      width="40%"
      mobileWidth="60%"
    />
  }
>
  C$\alpha$ (alpha-carbon), is the first carbon in the amino acid's [side
  chain](https://en.wikipedia.org/wiki/Side_chain); C$\beta$ (beta-carbon) is
  the second. Extremely simplistically, if we imagine a finger as a side chain,
  C$\alpha$ is at the base, C$\beta$ is closer to the tip. $d$ is the
  C$\beta$-C$\beta$ distance. Figure from
  <Reference id={1} />.
</Figure>

as well as 4 other angles:

<Figure
  content={
    <Image
      path={require("./images/interresidue-angles.png")}
      width="40%"
      mobileWidth="60%"
    />
  }
>
  Figure from <Reference id={1} />.
</Figure>

If we know these 6 numbers for each residue pair in folded 3D structure, then we should have a pretty good sense of what the structure looks like, setting a good foundation for step 2.

### The architecture

Here's the architecture of the trRosetta CNN. For our purposes, understanding the inner workings is not important, and we can just keep in mind the big picture: the network takes in an MSA and spits out these interresidue distances and orientation angles.

<Figure
  content={
    <Image
      path={require("./images/trRosetta-architecture.png")}
      width="40%"
      mobileWidth="60%"
    />
  }
>
  trRosetta uses a deep residual CNN. For more details, check out the [trRosetta
  paper](https://www.pnas.org/doi/10.1073/pnas.1914677117). Figure from{" "}
  <Reference id={1} />.
</Figure>

### Distance maps

We can represent the interresidue distances predicted by the network in a matrix called the **distance map**:

<Figure
  content={
    <Image
      path={require("./images/distance-map.png")}
      width="60%"
      mobileWidth="80%"
    />
  }
/>

Around the diagonal of the matrix are residues that are close in position, which are of course close in 3D space, hence the dark diagonal line. (Only the residues that are far apart in sequence but close in 3D are interesting and structure-defining.)

Neural networks output probabilities, not definitive answers. For example, language models like GPT – tasked with predicting the next word given some previous words as context – outputs a probability distribution over the set of all possible words (the vocabulary); in an additional final step, the word with the highest probability is chosen to be the prediction. In our case, trRosetta outputs probabilities for different distance bins, like this:

<table style={{width: 300, margin: "auto"}}>
  <tr>
    <th style={{textAlign: "left"}}>Distance bin</th>
    <th style={{textAlign: "left"}}>Probability</th>
  </tr>
  <tr>
    <td>0 - 0.5 $\text{\r{A}}$</td>
    <td>0.0001</td>
  </tr>
  <tr>
    <td>0.5 - 1 $\text{\r{A}}$</td>
    <td>0.0002</td>
  </tr>
  <tr>
    <td>...</td>
    <td>...</td>
  </tr>
  <tr>
    <td>5 - 5.5 $\text{\r{A}}$</td>
    <td>0.01</td>
  </tr>
  <tr>
    <td>5.5 - 6.0$\text{\r{A}}$</td>
    <td>0.74</td>
  </tr>
  <tr>
    <td>6.0 - 6.5$\text{\r{A}}$</td>
    <td>0.12</td>
  </tr>
  <tr>
    <td>...</td>
    <td>...</td>
  </tr>
  <tr>
    <td>19.5 - 20 $\text{\r{A}}$</td>
    <td>0</td>
  </tr>
</table>
<br />

In this example, it's pretty clear that trRosetta thinks the distance between these two residues should be around 6.0$\text{\r{A}}$, which we can use as our prediction. Because trRosetta is so confident, we say that the distance map is _sharp_.

But trRosetta is not always so confident. If the probability distribution is more uniform, it wouldn't be so clear which distance bin is best. In those cases, the distance map is _blurry_.

Let's visualize this. In the two distance maps we showed above, the colors reflect, for each residue pair, the sum of trRosetta's predicted probabilities for the bins in the $ <10 \text{\r{A}}$ range, i.e. how likely trRosetta thinks it is for the residues to end up close together in the 3D structure.

<Figure
  content={
    <Image
      path={require("./images/distance-map.png")}
      width="60%"
      mobileWidth="80%"
    />
  }
/>

The left distance map is blurry, while the right one is sharp.

If we provide trRosetta a garbage sequence that doesn't even encode a stable protein, no matter how good trRosetta is at its job of predicting distances, the distance map will be blurry; after all, how can trRosetta be sure if we ask for the impossible? Conversely, if we provide good sequences of stable proteins, trRosetta will produce sharp distance maps.

This idea is important because sharpness, like the monkey's excitement for bananas, is a signal that we can rely on to discriminate good sequences from bad ones.

### Quantifying sharpness

Leo Tolstoy famously said:

> All happy families are alike; each unhappy family is unhappy in its own way.

For distances maps produced by trRosetta, it's quite the opposite: all blurry distance maps are alike; each sharp distance map is sharp in its own way. Each functional protein has a unique structure – that determines a specific function – something that trRosetta learns to capture, whereas each nonfunctional sequence is kinda the same to trRosetta: a whole lotta garbage.

Without a canonical example of sharpness, the best way to quantify it is to come up with some blurry distance map $Q$ – a bad example – to steer away from: a distance map is sharp if it's very _different_ from $Q$.

To get $Q$, we can train trRosetta on the same data, except with one catch: we hide the identity of each amino acid. This retains some rudimentary characteristics of an amino acid chain, e.g. residues that are close in sequence are close in space. But it eliminates all information about the interactions between amino acids enabled by their unique chemistries.

Given some distance map $P$, how do we measure its similarity to our bad example, $Q$? Remember, a distance map is just a collection of probability distributions, one for each residue pair. If we can measure the difference in the probability distributions at each position – $P_{ij}$ and $Q_{ij}$ – we can average over those measurements and get a measurement between $P$ and $Q$:

$$
D_{\text{map}}(P, Q) = \frac{1}{L^2} \sum_{i, j = 1}^L D_{\text{distribution}}(P_{ij}, Q_{ij})
$$

where $L$ is the length of the sequence, $D_{\text{map}}$ measures similarity between distance maps, and $D_{\text{distribution}}$ measures similarity between probability distributions.

Here's one way to measure the similarity between two distributions:

$$
D_{\text{distribution}}(P_{ij}, Q_{ij}) = \sum_{x \in \text{bins}} P_{ij}^{(x)} \log \left(\frac{P_{ij}^{(x)}}{Q_{ij}^{(x)}}\right)
$$

This $D_{\text{distribution}}$ is called the **Kullback–Leibler (KL) divergence**, which came from [information theory](https://en.wikipedia.org/wiki/Information_theory). It's a common [loss function](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html) in machine learning. (Note on notation: it's conventional to use double bars like $D_{KL}(P || Q)$ to emphasize the fact that $D_{KL}(P || Q) \neq D_{KL}(Q || P)$, i.e. KL divergence is not a true distance metric.)

To summarize, we have developed a way to quantify the sharpness of a distance map $P$:

$$
D_{KL}(P || Q) = \frac{1}{L^2} \sum_{i, j = 1}^L \sum_{x \in \text{bins}} P_{ij}^{(x)} \log \left(\frac{P_{ij}^{(x)}}{Q_{ij}^{(x)}}\right)
$$

(Notation is slightly rewritten to be consistent with [this paper](https://www.nature.com/articles/s41586-021-04184-w)). Intuitively, $P$ is sharp if it's as far away from $Q$ as possible, as measured by the average KL divergence.

## Hallucinating proteins

To recap, when fed an amino acid sequence that encode a functional protein, trRosetta produces a sharp distance map, a good foundation for structure prediction.

<Figure
  content={<Image path={require("./images/hallucination-background.png")} />}
/>

When fed a random amino acid sequence, trRosetta produces a blurry distance map. But, equipped with a tool to measure sharpness, _we can sharpen the blurry distance map using MCMC_.

<Figure
  content={<Image path={require("./images/hallucination-MCMC-overview.png")} />}
/>

Let's start with a random sequence analogous to a white-noise image. At each MCMC step:

1. Make a random mutation in the sequence.
2. Feed the sequence into trRosetta to produce a distance map $P$.
3. Compare $P$ to $Q$, the blurry distance map generated by hiding amino acid identities.
4. Accept the mutation with high probability if it is a move in the right direction: maximizing the average KL divergence between $P$ and $Q$.
   - this acceptance criterion is called the [Metropolis criterion](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)
   - an additional parameter, $T$, is introduced as a knob we can use to control acceptance probability

<Figure
  content={<Image path={require("./images/hallucination-MCMC-details.png")} />}
/>

As we repeat these steps, the distance maps get progressively sharper, converging on a final, sharp distance map after 40,000 iterations.

<Figure
  content={
    <Image path={require("./images/hallucination-MCMC-progression.png")} />
  }
/>

When expressed in _E. coli_, many of these these hallucinated sequences folded into stable structures that closely match trRosetta's predictions.

<Figure
  content={
    <Image path={require("./images/hallucination-structure-example.png")} />
  }
/>

I find this astonishing. We can create stable proteins that have never existed in nature, guided purely by some information that trRosetta has learned about what a protein _should_ look like.

## Can we do better than MCMC?

MCMC is fundamentally inefficient. We're literally making random changes to see what sticks. Can we make more informed changes, perhaps using some directional nudges from the knowledgeable trRosetta?

There's just the thing in deep neural networks like trRosetta: **gradients**. During training, gradients guide trRosetta in adjusting its parameters to make better structure predictions. (If you're new to deep learning, I highly recommend this [video](https://youtu.be/VMj-3S1tku0?si=QKn0kH44GPjA1eBG) by Andrej Karpathy explaining gradients from the ground up.)

We already have a loss function: our average KL divergence between $P$ and $Q$. At each step:

1. Ask the differentiable trRosetta to compute gradients with respect to the loss.
2. Use the gradients to propose a mutation instead of using a random one.
   - Turning the gradients into a proposed mutation takes a few simple steps (bottom left of the diagram). They are explained in the methods section of [here](https://www.pnas.org/doi/10.1073/pnas.2017228118).

<Figure
  content={<Image path={require("./images/hallucination-gradients.png")} />}
/>

(This paper focuses on more constrained version of protein design called **fixed-backbone design**, which seeks an amino acid sequence given a target structure. This is why the loss function, in addition to the KL divergence term, also contains a term measuring the distance from the target structure (right of the diagram). The principles of leveraging gradients to create more informed mutations are the same, regardless of whether we have a target structure.)

Using this gradient-based approach, we can often converge to a sharp sequence map with much fewer steps, usually hundreds instead of tens of thousands.

## Designing useful proteins

So far, we have focused on designing stable proteins that fold into well-predicted structures. Let's extend this approach to designing proteins that have a desired function, such as binding to a therapeutically relevant target protein.

### Functional sites

Most proteins perform their function via a **functional site** formed by a small subset of residues (a **motif**). For example, the functional sites of enzymes, also called **active sites**, bind to their substrates and perform the catalytic function.

<Figure
  content={
    <Image
      path={require("./images/enzyme-active-site.png")}
      width="60%"
      mobileWidth="85%"
    />
  }
>
  https://biocyclopedia.com/index/general_zoology/action_of_enzymes.php
</Figure>

Since it's really the functional site that matters, a natural problem is: given a desired functional site, can we design a protein that contains it? This is often called **scaffolding** a functional site. Solutions to this problem has wide-ranging implications, from designing new vaccines to interfering with cancer.

<Figure
  content={
    <Image
      path={require("./images/scaffolding-motif.png")}
      width="40%"
      mobileWidth="60%"
    />
  }
></Figure>

### Satisfying the motif

To guide MCMC towards sequences containing the desired motif, we can introduce an additional term to our loss function to capture motif satisfaction:

$$
Loss = Loss_{FH} + Loss_{MS}
$$

where $Loss_{FH}$, the **free-hallucination loss**, is our average DL divergence from before, nudging the model away from $Q$ to be more generally protein-like; and $Loss_{MS}$ is the new **motif-satisfaction loss**.

Intuitively, this loss needs to be small when the structure predicted by trRosetta clearly contains the desired motif – and big otherwise. For the mathematical details, check out the methods section [here](https://www.biorxiv.org/content/10.1101/2020.11.29.402743v1). We are engaging in a balancing act: we want proteins that contain the functional site (low motif-satisfaction loss) that are also generally good, stable proteins (low free-hallucination loss)!

<Figure
  content={
    <Image
      path={require("./images/motif-satisfaction-overview.png")}
      width="60%"
      mobileWidth="85%"
    />
  }
></Figure>

### A case study: SARS-CoV-2

SARS-CoV-2, the virus behind the Covid-19 pandemic, has a clever way of entering our cells. It takes advantage of an innocent, blood-pressure regulating protein in our body called **angiotensin-converting enzyme 2 (ACE2)** that straddles the cell membrane.

<Figure
  content={
    <Image path={require("./images/ACE2.png")} width="50%" mobileWidth="75%" />
  }
></Figure>

It anchors itself by binding to an [alpha helix](https://en.wikipedia.org/wiki/Alpha_helix) in ACE2 exposed to the outside, and then enters the cell:

<Figure
  content={<Image path={require("./images/ACE2-attacked.png")} />}
></Figure>

One way we can disrupt this mechanism is to _design a protein that contains ACE2's interface alpha helix_. Our protein would trick the coronavirus into thinking that it is ACE2 – and bind to it instead – sparing our innocent ACE2's.
These therapeutic proteins are called **receptor traps**: they trap the receptors on the coronavirus spike protein.

This is exactly our functional site scaffolding problem. Folks at the Baker lab used the composite loss function approach and hallucinated these receptor traps containing the interface helix (shown on the right).

<Figure
  content={<Image path={require("./images/ACE2-designs.png")} width="80%" />}
></Figure>

I hope I have convinced you that these hallucinations are not only cool but also profoundly useful. It was only in the past few years that we have acquired the ability to engineer proteins with this level of control, thanks primarily to groundbreaking work from the Baker lab.

And of course, this is only the tip of the iceberg: the ability to engineer proteins that disrupt disease mechanisms will revolutionize drug discovery and reduce a lot of suffering in the world.

## Final notes

- Throughout this post, we exclusively focused on the distances produced by trRosetta, compiled in distance maps. There are also the 5 angles parameters that work in the exact same way: binned predictions, KL divergence, etc. trRosetta outputs 1 distance map and 5 "angle"-maps, all of which are used to drive the hallucinations.

- trRosetta is no longer the best structure prediction model, a testament to this rapidly moving field. Since 2021, two models have consistently demonstrated superior performance: AlphaFold from DeepMind and RoseTTAFold from the Baker lab.
  - Both AlphaFold and RoseTTAFold are deep neural networks, so all the ideas discussed in this post still apply.
  - [This paper](https://onlinelibrary.wiley.com/doi/full/10.1002/pro.4653) applies the same techniques using AlphaFold; many subsequent papers from the Baker lab use RoseTTAFold instead of trRosetta, including the one that designed the SARS-CoV-2 receptor trap.

## References

<ReferenceList />
<NoteList />
