---
title: "Protein VAEs"
date: "2024-01-15"
description: Stuff
---

import Figure from "../../../src/components/figure.jsx"
import Image from "../../../src/components/image.jsx"
import LinkPreview from "../../../src/components/link-preview.jsx"
import { Link } from "gatsby"
import MSACoupling from "../protein-evolution/d3/MSACoupling.jsx"
import { Note, NoteList } from "./Notes.jsx"

Life, in essence, is a dizzying chemical dance choreographed by proteins. It's so mind-bendingly complex that most of its patterns still elude us. This blog is about the inadequate but inspiring ways we have of shedding light on this elusive dance – of understanding the methods in the madness. Here's one such method:

**Binding pockets** are "hands" that proteins use to act on their surroundings: [speed something up](https://en.wikipedia.org/wiki/Enzyme), [break something down](https://en.wikipedia.org/wiki/Protease), [guide something along](<https://en.wikipedia.org/wiki/Chaperone_(protein)>).

<Figure
  content={
    <Image
      path={require("./images/binding-site.png")}
      width="40%"
      mobileWidth="60%"
    />
  }
>
  A binding pocket. Image from
  [https://en.wikipedia.org/wiki/Binding_site](https://en.wikipedia.org/wiki/Binding_site).
</Figure>

Over billions of years, evolution introduces random mutations to every protein. There is a pattern: the binding pockets almost never change. This is perhaps unsurprising: they are the parts that actually do the work! Spoons come in different shapes and sizes, but the part that scoops never changes.

<Figure
  content={
    <Image
      path={require("./images/spoons.png")}
      width="50%"
      mobileWidth="60%"
    />
  }
/>

That's why the evolutionary history of a protein, in the form of a [Multiple Sequence Alignments (MSAs)](https://en.wikipedia.org/wiki/Multiple_sequence_alignment), holds such important clues to the protein's structure and function – its role in this elusive dance. Positions that correlate in the MSA tend to have some important relationship with each other, e.g. direct contact in the folded structure.

<Figure content={<MSACoupling />}>
  An MSA contains many different variants of a protein sequence sampled by
  evolution. The structure sketches how the amino acid chain might fold in
  space. Hover over each row in the MSA to see the corresponding amino acid in
  the folded structure. Hover over the blue link to highlight the contacting
  positions.
</Figure>

A possible explanation: these correlated positions form a binding pocket with some important function. A willy-nilly mutation to one position disrupts the binding pocket and renders the protein useless. During evolution, poor organisms that carried that mutation didn't survive and are therefore absent from the MSA.

In a previous <Link to="/protein-evolution">post</Link>, we talked about ways of teasing out such information from MSAs using [pair-wise models](https://en.wikipedia.org/wiki/Potts_model) that account for every possible pair of positions. But what about the interactions between 3 positions? Or even more? Binding pockets, after all, are made up of many positions. Unfortunately, accounting for all the possible combinations in this way is computationally impossible.

This post is about a solution to this problem of having to track these far-too-numerous combinations – using a tool from machine learning called **variational autoencoders (VAEs)**. If you're new to VAEs, check out this deep dive!

<LinkPreview
  title="An introduction to variational autoencoders"
  description="Predicting protein function using deep generative models. Latent variable models, reconstruction, variational autoencoders (VAEs), Bayesian inference, evidence lower bound (ELBO)."
  url="https://liambai.com/variational-autoencoder"
  ogImageSrc="https://liambai.com/previews/variational-autoencoder.png"
/>

## The idea

### Latent variables

Imagine some vector $\mathbf{z}$, a **latent variable**, that distills all the information in the MSA. All the interactions: pairwise, any 3 positions, any 4... Knowing $\mathbf{z}$, we'd have a pretty good idea about the important characteristics of our protein.

<Figure
  content={
    <Image path={require("../variational-autoencoder/images/MSA-latent.png")} />
  }
>
  Applying latent variable models like VAEs to MSAs. TODO: cite.
</Figure>

We can view $\mathbf{z}$ as a form of data compression: the piles of sequences in this MSA into one small vector <Note id={1} />. Here's the key insight of VAEs: we might not actually know what the best way of doing this compression is; let's ask neural networks to figure it out. The neural network that creates $\mathbf{z}$ is called an **encoder**.

### VAEs in a nutshell

Given a protein sequence, let's ask the encoder: can you capture (in $\mathbf{z}$) its salient features? Things like: which residues form a binding pocket. There are 2 rules:

1. No BS. You have to actually capture something meaningful about the input sequence. As a test, a neural network (called a **decoder**) needs to be able to tell from $\mathbf{z}$ what the input sequence was, reasonably well. This rule is called **reconstruction**.

2. No rote memorization. If you merely memorize the input sequence, you'll be great at reconstruction but you'll be stumped by sequences you've never seen before. This rule is called **regularization**.

A common theme of machine learning is the tension between these to rules – the need to artfully balance them. For VAEs, they define the two terms of the <Link to="/variational-autoencoder/#the-loss-function">loss function</Link> we use when training.

<Figure
  content={
    <Image
      path={require("../variational-autoencoder/images/VAE-compression.png")}
      width="60%"
    />
  }
>
  Variational autoencoders are a type of encoder-decoder model. Figure from this
  [blog
  post](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73).
</Figure>

### The model

OK, intuition aside, what do we actually mean by _model_? What are its inputs and outputs? Concretely, our model is just a function that takes a protein sequence, say ILAVP, and spits out a probability, $p(\mathrm{ILAVP})$:

$$
\mathrm{ILAVP} \rightarrow p(\mathrm{ILAVP})
$$

With training, we want this probability to approximate how likely it is for ILAVP to have come from our MSA. In other words, how likely is it to be a functional variant of our protein?

This probability is the collaborative work of our encoder and decoder, which are trained together.

$$
\mathrm{ILAVP} \xrightarrow{encoder} \mathbf{z} \xrightarrow{decoder} p(\mathrm{ILAVP})
$$

An accurate model is powerful. It enables us to make predictions about protein variants we've never seen before – including ones associated with disease – or even engineer new ones with properties we desire. More on this later!

### Training & inference

Training our model looks something like this:

1. take an input sequence, say ILAVP, from the MSA.
2. pass it through encoder and decoder: $\mathrm{ILAVP} \xrightarrow{encoder} \mathbf{z} \xrightarrow{decoder} p(\mathrm{ILAVP}).
$
3. compute the loss function.
4. use gradient descent to update the encoder and decoder parameters (purple arrow).
5. repeat.

After going through each sequence in the MSA, our model should have a decent idea of what it's like to be this protein!

Now, when given an unknown input sequence, we can pass it through the VAE in the same way and produce an informed probability for the input sequence (green arrow).

<Figure
  content={
    <Image
      path={require("./images/protein-vae-architecture.png")}
      width="90%"
    />
  }
></Figure>

Once training, we can think of our model's predictions, e.g. $p(\mathrm{ILAVP})$, as a measure of the fitness.

- $p(\mathrm{ILAVP})$ is low $\rightarrow$ ILAVP probably won't even fold into a working protein.
- $p(\mathrm{ILAVP})$ is high $\rightarrow$ ILAVP fits right in with the natural variants of this protein – and probably works great.

Now, let's put our model to use.

## VAEs at work

### Predicting disease variants

The explosion in DNA sequencing technology in the last decade came with a conundrum: the enormous amounts of sequence data we unlocked far exceeds our ability to understand them.

For example, [genomAD](https://gnomad.broadinstitute.org/) is a massive database of sequence data. If we look at all the human protein variants in genomAD and ask: for how many of these do we know their disease consequences? The answer: a mere 2%. We are deeply ignorant about the proteins in our bodies and how their malfunctions cause disease.

Now, image a tool that can look at every of possible variant of a protein and make a prediction about its consequence. Something like this, where blue means good and red means bad.

<Figure
  content={<Image path={require("./images/mutation-effect-heatmap.png")} />}
>
  TODO
</Figure>

A map like this, if dependable, so valuable precisely because of our lack of experimental data. It fills the gaps in our understanding.

### The log ratio

Here's how we can compute such a map like that with our VAE. Given a natural sequence (called wild-type) and a mutant sequence, the log ratio

$$
\log\frac{p(\text{mutant})}{p(\text{wild-type})}
$$

measures the improvement of the mutant over the wild-type <Note id={2} />.

- If our model favors the mutant over the wild-type $\rightarrow$ $p(\text{mutant}) > p(\text{wild-type})$ $\rightarrow$ positive log ratio $\rightarrow$ the mutation is likely beneficial.

- If our model favors the wild-type over the mutant $\rightarrow$ $p(\text{wild-type}) > p(\text{mutant})$ $\rightarrow$ negative log ratio $\rightarrow$ the mutation is likely harmful.

We can create our map by simply computing this log ratio for every possible mutation at every possible position.

### Evaluating our predictions

[ClinVar](https://www.ncbi.nlm.nih.gov/clinvar/intro/) is a smaller database of mutation effects in proteins important to human health. We can evaluate our model's performance by comparing its predictions to the actual outcomes recorded in ClinVar.

<Figure content={<Image path={require("./images/EVE-ClinVar.png")} />}>
  EVE outperforms other computational approaches of variant effect prediction in
  concordance with ClinVar. On the y-axis, [Deep Mutational Scanning
  (DMS)](https://www.nature.com/articles/nmeth.3027) is an experimental method
  for screening a large set of variants for a specific function, another source
  of experimental labels. When evaluated on the concordance with DMS data, EVE
  similarly outperforms others models.
</Figure>

On ClinVar, along with another experimental dataset of mutation effects, a VAE-based model called [EVE](https://github.com/OATML/EVE) did better than all previous models.

Incredibly, VAE models acquire such strong predictive power while being completely **unsupervised**! They had never seen _any_ labeled data of mutation effects – and learned purely through studying the evolutionary sequences in the protein's family.

### Predicting viral antibody escape

A costly challenge during the COVID pandemic was the constant emergence of viral variants that evolved to escape our immune system, a phenomenon known as **antibody escape** (antibodies are our bodies' natural fighters that fend off the virus; vaccines work by boosting their presence).

Could we have flagged such dangerous variants ahead of their breakout? Such early warnings would have won life-saving time for vaccine development.

The VAE-based model EVE can help do exactly that. [EVEScape](https://evescape.org/) combines EVE's mutation fitness predictions with biophysical data to achieve accurate predictions of antibody escape.

<Figure content={<Image path={require("./images/EVEScape.png")} />}>
  TODO
</Figure>

Had we known...

<Figure content={<Image path={require("./images/EVEScape-timeline.png")} />}>
  TODO
</Figure>

## The power of latent variables

When compared to independent position-wise models, the latent variable model is significantly more accurate for all possible mutations.

<Figure
  content={<Image path={require("./images/DeepSequence-vs-others.png")} />}
></Figure>

The sites at which their accuracy improved the most are ones that cooperate with several other sites.

<Figure
  content={<Image path={require("./images/DeepSequence-vs-independent.png")} />}
></Figure>

This demonstrates the flexibility of the latent variable model that can capture higher-order interactions.

Here's one way to look at these results. MSAs contain a wealth of information, some of which we can understand through simple statistics: <Link to="/protein-evolution/#counting-amino-acid-frequencies">position-wise frequencies</Link>, <Link to="/protein-evolution/#pairwise-frequencies">pair-wise frequencies</Link>, etc. Those models are interpretable but limiting – they fail at teasing out more complex, higher-order signals.

Enter neural networks, which are much better than us at recognizing those signals hidden in MSAs. They known _where to look_, _what to look at_ – beyond our simple statistics. This comes at the cost of interpretability. The comparison between our VAE model and statistical models like <Link to="/protein-evolution">Direct Coupling Analysis (DCA)</Link> exemplifies these two paradigms.

<NoteList />
