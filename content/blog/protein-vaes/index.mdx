---
title: "Protein VAEs"
date: "2024-01-15"
description: Stuff
---

import Figure from "../../../src/components/figure.jsx"
import Image from "../../../src/components/image.jsx"
import LinkPreview from "../../../src/components/link-preview.jsx"
import { Link } from "gatsby"
import MSACoupling from "../protein-evolution/d3/MSACoupling.jsx"

The evolutionary history of a protein – in the form of a [Multiple Sequence Alignments (MSAs)](https://en.wikipedia.org/wiki/Multiple_sequence_alignment) - holds clues to its structure and function. An example: positions that correlate tend to have some important relationship with each other, e.g. direct contact in the folded structure.

<Figure content={<MSACoupling />}>
  An MSA contains many different variants of a protein sequence sampled by
  evolution. The structure sketches how the amino acid chain might fold in
  space. Hover over each row in the MSA to see the corresponding amino acid in
  the folded structure. Hover over the blue link to highlight the contacting
  positions.
</Figure>

Why? One possible explanation: these correlated positions form a binding pocket important to the protein's function. A willy-nilly mutation to one position disrupts the binding pocket and renders the protein useless. Poor organisms that carried that mutation are dead therefore absent from the MSA.

<Figure
  content={
    <Image
      path={require("./images/binding-pocket.png")}
      width="40%"
      mobileWidth="60%"
    />
  }
>
  A binding pocket. TODO: cite.
</Figure>

In a previous <Link to="/protein-evolution">post</Link>, we talked about ways of capturing information like this hidden in MSAs using [pair-wise models](https://en.wikipedia.org/wiki/Potts_model) that account for every possible pair of positions. But what about the interactions between 3 positions? Or even more? Binding pockets, for example, are made up of many positions. Unfortunately, accounting for all these possible combinations is computationally impossible.

This post is about a solution to this problem of tracking these far-too-numerous combinations – based on a machine learning tool called **variational autoencoders (VAEs)**. If you're new to VAEs, check out this deep dive.

<LinkPreview
  title="An introduction to variational autoencoders"
  description="Predicting protein function using deep generative models. Latent variable models, reconstruction, variational autoencoders (VAEs), Bayesian inference, evidence lower bound (ELBO)."
  url="https://liambai.com/variational-autoencoder"
  ogImageSrc="https://liambai.com/previews/variational-autoencoder.png"
/>

## The idea

### Latent variables

Imagine some vector $\mathbf{z}$, a **latent variable**, that distills all the information in the MSA: pairwise, higher-order, anything.

<Figure
  content={
    <Image path={require("../variational-autoencoder/images/MSA-latent.png")} />
  }
>
  Applying latent variable models like VAEs to MSAs. TODO:
  https://en.wikipedia.org/wiki/Binding_site.
</Figure>

How do we get this magical $\mathbf{z}$? Since we can't precisely account all the combinations, let's ask a neural network, called an **encoder**, to figure it out. That's the key idea behind VAEs.

### VAEs in a nutshell

Given a protein sequence, let's ask the encoder: Can you capture (in $\mathbf{z}$) the salient features of this protein? Biologically, this might mean things like: which residues form a binding pocket, or interact allosterically. There are 2 rules:

1. No BS. You have to actually capture something meaningful about the input sequence. As a test, a neural network (called a **decoder**) needs to be able to tell from $\mathbf{z}$ what the input sequence was, reasonably well. TODO: note on joint training. This is called **reconstruction**.

2. No rote memorization. If you merely memorize the sequence, you'll be great at the first rule but you won't be any help for sequences you've never seen before. This is called **regularization**.

These 2 rules define the <Link to="/variational-autoencoder/#the-loss-function">loss function</Link> we use when training the neural networks.

<Figure
  content={
    <Image
      path={require("../variational-autoencoder/images/VAE-compression.png")}
      width="60%"
    />
  }
>
  Variational autoencoders are a type of encoder-decoder model. Figure from this
  [blog
  post](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73).
</Figure>

A note on the decoder.

## Training & inference

During training, we show the MSA to our VAE one sequence at a time. For each sequence, we compute the loss function and use gradient descent to update the encoder and decoder parameters.

After seeing everything in the MSA, hopefully our VAE has learned some important things about what's it's like to be this protein, e.g. which residues form a binding pocket.

To train the VAE, we feed in sequences in the MSA and use gradient descent to update the encoder and decoder parameters (blue arrow). During inference – i.e. when given an unknown input sequence – we pass it through the encoder-decoder in the same way and produce a probability for the input sequence (green arrow).

<Figure
  content={
    <Image
      path={require("./images/DeepSequence-architecture.png")}
      width="90%"
    />
  }
></Figure>

Given an unknown input sequence $\mathbf{x}$, $p(\mathbf{x})$ describes how likely it is for $\mathbf{x}$ to be a functional protein given the evolutionary sequences in our MSA. $p(\mathbf{x})$ is a fitness predictor: high = good, low = bad. from improving protein function to interpreting human disease variants.

## Predicting mutation effects

Given a wild-type sequence $\mathbf{x}^{\text{(Wild-type)}}$ and a mutant sequence $\mathbf{x}^{\text{(Mutant)}}$, the log ratio

$$
\log\frac{p(\mathbf{x}^{\text{(Mutant)}})}{p(\mathbf{x}^{\text{(Wild-type)}})}
$$

measures the improvement of the mutant over the wild-type.

<>
  5: (
  <>
    <span>This is equivalent to the equations we derived </span>
    <a href="https://liambai.com/protein-evolution/#predicting-mutation-effects">
      here
    </a>
    <span> using a pair-wise model.</span>
  </>
  )
</>

- Our model favors $\mathbf{x}^{\text{(Mutant)}}$ over $\mathbf{x}^{\text{(Wild-type)}}$ $\rightarrow$ $p(\mathbf{x}^{\text{(Mutant)}}) > p(\mathbf{x}^{\text{(Wild-type)}})$ $\rightarrow$ the mutation is likely beneficial $\rightarrow$ positive log ratio.

- Our model favors $\mathbf{x}^{\text{(Wild-type)}}$ over $\mathbf{x}^{\text{(Mutant)}}$
  $\rightarrow$ $p(\mathbf{x}^{\text{(Wild-type)}}) > p(\mathbf{x}^{\text{(Mutant)}})$ $\rightarrow$ the mutation is likely harmful $\rightarrow$ negative log ratio.

### Disease variants

We can compute this log ratio for each possible amino acid substitution at each position to produce a heatmap that illustrates their consequences:

<Figure
  content={<Image path={require("./images/mutation-effect-heatmap.png")} />}
></Figure>

Predicting the pathogenicity of human protein variants like this can be an important guide for clinical decisions. Most of the variants of human proteins related to disease (98% of [genomAD](https://gnomad.broadinstitute.org/)!) have unknown consequences, a testament to <Link to="/protein-representation">the difficulty of obtaining functional data relative to sequence data</Link>. This means:

1. we are deeply ignorant about the proteins in our bodies and how their malfunctions cause disease.

2. unsupervised computational approaches like VAEs can make a huge impact.

Among this scarcity of experimental data, [ClinVar](https://www.ncbi.nlm.nih.gov/clinvar/intro/) is a public database of mutation effects in proteins important to human health. Predictions from a VAE-based approach called [EVE](https://github.com/OATML/EVE) agrees strongly with labels from ClinVar.

<Figure content={<Image path={require("./images/EVE-ClinVar.png")} />}>
  EVE outperforms other computational approaches of variant effect prediction in
  concordance with ClinVar. On the y-axis, [Deep Mutational Scanning
  (DMS)](https://www.nature.com/articles/nmeth.3027) is an experimental method
  for screening a large set of variants for a specific function, another source
  of experimental labels. When evaluated on the concordance with DMS data, EVE
  similarly outperforms others models.
</Figure>

Incredibly, VAE models acquire such strong predictive power while being completely unsupervised: they has never seen _any_ labeled data of variant effects. They learn purely through studying the evolutionary sequences in the protein's family.

### The power of latent variables

When compared to independent position-wise models, the latent variable model is significantly more accurate for all possible mutations.

<Figure
  content={<Image path={require("./images/DeepSequence-vs-others.png")} />}
></Figure>

The sites at which their accuracy improved the most are ones that cooperate with several other sites.

<Figure
  content={<Image path={require("./images/DeepSequence-vs-independent.png")} />}
></Figure>

This demonstrates the flexibility of the latent variable model that can capture higher-order interactions.

Here's one way to look at these results. MSAs contain a wealth of information, some of which we can understand through simple statistics: <Link to="/protein-evolution/#counting-amino-acid-frequencies">position-wise frequencies</Link>, <Link to="/protein-evolution/#pairwise-frequencies">pair-wise frequencies</Link>, etc. Those models are interpretable but limiting – they fail at teasing out more complex, higher-order signals.

Enter neural networks, which are much better than us at recognizing those signals hidden in MSAs. They known _where to look_, _what to look at_ – beyond our simple statistics. This comes at the cost of interpretability. The comparison between our VAE model and statistical models like <Link to="/protein-evolution">Direct Coupling Analysis (DCA)</Link> exemplifies these two paradigms.
